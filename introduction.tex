% (This file is included by thesis.tex; you do not latex it by itself.)

\begin{introduction}

    % If you need to use a \section
    % command you will need to use \section*, \subsection*, etc. so that
    % you don't get any numbering.  You probably won't be using any of
    % these commands in the abstract anyway.
    % \section{California's Transfer Maze: An Archetype for a National Problem}

    California's public higher education system is a titan of American academia, a complex, three-tiered structure comprising the University of California (UC), California State University (CSU), and the California Community Colleges (CCC)~\cite{ppic}. Collectively, these 149 colleges and universities serve nearly 2.9 million students, forming the largest public higher education system in the United States~\cite{ppic,uc,calstate,cccco}. A foundational principle of this system is the promise of student mobility, particularly the pathway from a two-year community college to a four-year university. However, the mechanism designed to facilitate this movement, the process of determining course equivalency, or articulation, is a formidable, largely manual process that creates significant barriers for students.

    At the heart of this process is the Articulation System Stimulating Interinstitutional Student Transfer (ASSIST), the state's official public repository for articulation agreements~\cite{assistinfo}. While ASSIST provides a centralized platform for students and advisors to view established equivalencies, it is fundamentally a display for agreements that are negotiated and updated manually by articulation officers at each individual campus~\cite{assistfaq}. Given the sheer number of institutions, the process of defining and maintaining these agreements is a task of bleak combinatorics, rendering it inefficient, slow, and inherently intractable~\cite{pardos2019}. This manual paradigm places a considerable burden on academic advisors and administrative staff, who must meticulously review course descriptions and syllabi to compare content, rigor, and learning outcomes~\cite{pardos2019}. The result is a system that struggles to keep pace with the needs of a vast and mobile student body, making California a critical case study for a problem that extends far beyond its borders.

    The challenges exemplified by California's system are a microcosm of a systemic crisis in American higher education. The act of transferring between institutions has become a normative component of the modern student's academic journey. Data from the National Student Clearinghouse Research Center reveals that in the fall of 2023, transfer enrollment constituted 13.2\% of all continuing and returning undergraduates~\cite{nscnews2023}. This trend is not static; it represents a post-pandemic resurgence in student mobility, with transfer enrollment growing by 5.3\% from Fall 2022 to Fall 2023 and an additional 4.4\% in Fall 2024~\cite{nscnews2023,nscnews20250305}. This mobile population is increasingly diverse, comprising not only students following traditional two-year to four-year pathways but also a substantial number of returning learners who have previously paused their education. Over half of these returning students opt to re-enroll at a new institution, underscoring the critical role of the transfer system in providing flexible pathways to degree completion~\cite{nscdd20250507}.

    The consequences of this systemic inefficiency are borne almost entirely by the students, manifesting in significant academic and financial setbacks. The most direct and damaging outcome is the loss of earned academic credit. A comprehensive 2017 report by the U.S. Government Accountability Office (GAO) estimated that students who transferred between 2004 and 2009 lost an average of 43\% of their credits in the process~\cite{gao2017}. This finding is echoed across numerous studies, with reports indicating that more than half of all transfer students lose at least some credits, and approximately one-fifth are forced to repeat courses for which they have already received a passing grade at a previous institution~\cite{publicagenda2025}.

    This loss of credit creates a cascade of negative consequences. It invariably leads to an increased time-to-degree, delaying graduation and entry into the workforce. Each repeated course also carries a financial cost, increasing the total tuition burden and potentially exhausting a student's eligibility for federal financial aid programs like Pell Grants and Direct Loans~\cite{gao2017}. A process that is often undertaken to save money (for example, by starting at a less expensive community college) can paradoxically result in a greater overall financial commitment, trapping students in a cycle of additional coursework and debt~\cite{collegeopportunity2017}.

    The friction and frustration inherent in the transfer process also have a measurable impact on student persistence and graduation. Studies have shown that transfer students, as a group, tend to have lower retention and graduation rates than their peers who begin and end their studies at the same institution~\cite{porter1999}. This issue transcends mere administrative inefficiency and becomes a critical matter of educational equity. Low-income students and students from historically underrepresented racial and ethnic groups are more likely to begin their postsecondary journey at community colleges and rely on transfer pathways to attain a bachelor's degree~\cite{ace2025}. The recent growth in transfer enrollment has been driven disproportionately by Black and Hispanic students~\cite{nscnews2023}. Therefore, the barriers imposed by an inefficient articulation system such as credit loss, increased cost, and delayed graduation, disproportionately harm the very student populations that institutions are striving to support.

    A clear and troubling feedback loop emerges from this analysis. The fundamentally manual and inefficient nature of course articulation is a direct cause of credit loss. This credit loss imposes a tangible academic and financial burden on students which falls most heavily on underrepresented and low-income students, who are a large and growing segment of the transfer population. This disproportionate impact, in turn, undermines institutional goals of improving student retention and closing persistent equity gaps in degree attainment. Thus, the seemingly low-level administrative task of determining course equivalency is revealed to be a significant driver of systemic inequity in higher education. Addressing this challenge through robust automation is not merely an operational optimization; it is a necessary intervention to foster a more equitable, efficient, and supportive educational ecosystem for all students.

    \section{The Call for Automation}\label{sec:automation}
    The manifest inefficiencies and inequities of manual course articulation, exemplified by the challenges within California's vast system, have prompted a range of research efforts aimed at automating the process~\cite{ma_course_recommendation_2017, PardosCourse2Vec2019, pardos-articulation-2019, JiangPardosMulti2VecEDM2020,XuPardosSubwordEmbeddings2024}. These technological interventions have evolved in sophistication, mirroring the broader advancements in natural language processing (NLP) and machine learning. A critical review of this literature reveals a clear trajectory from simple statistical methods to complex deep learning models, with each stage introducing new capabilities while also exposing new limitations. This evolution illuminates the path toward a more robust and scalable solution.

    \subsection{Foundational Approaches: Keyword and Statistical Methods}
    The earliest attempts at automating course comparison relied on foundational text analysis techniques that, while computationally simple, lack semantic depth. The most basic systems are essentially search engines or databases that depend on exact keyword matching or pre-populated tables of known equivalencies~\cite{shamrock}. These systems are inherently brittle; they cannot recognize semantic variations (e.g., equating ``Introduction to Programming'' with ``Fundamentals of Computer Science I'') and require continuous manual updates to remain relevant~\cite{shiferaw2024}.

    A more advanced statistical method, Term Frequency-Inverse Document Frequency (TF-IDF), improves upon keyword matching by vectorizing documents and weighting terms based on their importance. A term's frequency within a single document (TF) is balanced against its rarity across a collection of documents, or corpus (IDF)~\cite{AIZAWA200345}. This allows the model to assign higher importance to distinctive terms (e.g., ``calculus'') and lower importance to common words (e.g., ``the,'' ``a,'' ``is'')~\cite{AIZAWA200345}. TF-IDF has been a workhorse for information retrieval and has been applied to course similarity tasks~\cite{AIZAWA200345}. However, the fundamental limitation of TF-IDF and other bag-of-words models is their complete lack of semantic understanding. They treat words as discrete, unrelated tokens and cannot grasp that ``calculus'' and ``differentiation'' are related concepts, nor can they distinguish between different meanings of the same word.

    \subsection{Static Semantic Representations}
    The development of word embeddings represented the first major leap toward a true semantic understanding of course content. Models like Word2Vec and GloVe, trained on vast text corpora, learn to represent words as dense vectors in a high-dimensional space, where words with similar meanings are positioned closer to one another. For example, the vectors for ``car'' and ``automobile'' would be near each other, while being distant from the vector for ``planet.'' This innovation enabled a more nuanced comparison of texts than was possible with TF-IDF. In the context of educational data mining, these techniques have been applied to content-based course recommendation systems, typically by creating a single vector representation for a course description by averaging the vectors of all its constituent words~\cite{pardos10.1145/3330430.3333622}.

    Despite this advancement, these models produce static embeddings. Each word is assigned a single, fixed vector regardless of its context. This is a significant drawback, as it fails to account for polysemy: words with multiple meanings. For instance, the word ``bank'' would have the same vector in the phrases ``river bank'' and ``bank account,'' despite their disparate meanings. Furthermore, the common practice of averaging all word vectors to create a document-level representation is a crude heuristic that can dilute or lose critical semantic information, especially in complex or lengthy descriptions.

    \subsection{Contextual Semantic Representations}
    The introduction of the transformer architecture, and specifically models like Bidirectional Encoder Representations from Transformers (BERT), revolutionized NLP by enabling the generation of contextual embeddings. In these models, the vector representation of a word is dynamically influenced by the words surrounding it in a sentence~\cite{devlin2019bertpretrainingdeepbidirectional}. This allows the model to disambiguate word meanings and capture a much richer, more accurate semantic representation of the text. Architectures such as Sentence-BERT (SBERT) were subsequently developed to fine-tune these models specifically for the task of producing semantically meaningful embeddings for entire sentences or short paragraphs, which can then be efficiently compared using metrics like cosine similarity~\cite{reimers2019sentencebertsentenceembeddingsusing}.

    The most recent evolution in this domain involves the direct application of large-scale generative models, or Large Language Models (LLMs) like GPT-4 and Gemini, for classification tasks. Through sophisticated prompt engineering and in-context learning, these models can be instructed to perform pairwise comparisons of course descriptions and render a judgment on their equivalency. The preliminary work for this thesis explored this very approach, using Google's Gemini Pro to classify course pairs. While these experiments yielded promising accuracy, they also uncovered significant practical and theoretical limitations. The direct use of LLMs for this task is computationally expensive and inefficient, as it requires repeatedly sending full text descriptions to a model API for every comparison. Performance is acutely sensitive to the exact phrasing of the prompt, necessitating a costly and time-consuming iterative tuning process. Most critically, the decision-making process of an LLM is a ``black box,'' providing a categorical output (e.g. ``equivalent'') without a quantifiable similarity score or confidence level. This opacity makes it difficult to rank potential matches, set decision thresholds, or provide transparent justifications for the model's conclusions. This approach is also ill-suited for handling more complex articulation scenarios, such as one-to-many or many-to-many course mappings.

    \subsection{Enrollment-Based Approaches}
    It is essential to situate this research within the context of parallel efforts that leverage different data sources. A notable body of work has demonstrated that course similarity and prerequisite relationships can be predicted with high accuracy by analyzing student enrollment data. Models such as \emph{course2vec} learn course embeddings not from their textual descriptions, but from the patterns of which courses students tend to take together~\cite{pardos2018connectionistrecommendationwildutility}. The underlying principle is that courses frequently taken in the same semester or in sequence likely share a functional or topical relationship~\cite{pardos2018connectionistrecommendationwildutility}.

    While this behavioral approach is powerful, its reliance on large-scale, proprietary institutional datasets of student records presents two major obstacles. First, it raises significant data privacy and security concerns, as it involves the analysis of sensitive student information~\cite{slade10.1177/0002764213479366}. Second, it limits the scalability and generalizability of the solution. The model is only applicable at institutions that can provide access to such data, and it cannot be used to compare courses between two institutions that have no history of student transfer between them. This approach is therefore not a universal solution for the broader course articulation problem.

    The evolution of these varied approaches reveals a fundamental trade-off: as models gain greater semantic power, they tend to become more computationally intensive, less interpretable, and more demanding of specialized or private data. The limitations of direct LLM classification (cost, opacity) and enrollment-based methods (data privacy, limited access) point toward the need for a new paradigm. An ideal solution should harness the semantic power of large pre-trained models without inheriting their operational burdens. This suggests that the next logical step is not simply a larger, more complex end-to-end model, but rather a more intelligent, hybrid framework. Such a framework would decouple the task of deep semantic representation from the task of final classification, allowing each component to be optimized for what it does best. This conceptual shift forms the central motivation for the methodology proposed in this thesis.  Table~\ref{tbl:taxonomy} summarizes the primary methods for determining course transferability.

    \begin{table}[!htb]
        \centering
        \setlength{\tabcolsep}{10pt} % Default value: 6pt
        \renewcommand{\arraystretch}{1.5}  % Provide more space between table rows, if you prefer
        \caption{Comparative Taxonomy of Course Equivalency Determination Methods}\label{tbl:taxonomy}
        \resizebox{\columnwidth}{!}{
            \begin{tabular}{>{\raggedright\arraybackslash}p{2.25cm}>{\raggedright\arraybackslash}p{3cm}>{\raggedright\arraybackslash}p{1.75cm}>{\raggedright\arraybackslash}p{1.9cm}>{\raggedright\arraybackslash}p{3.75cm}>{\raggedright\arraybackslash}p{3.75cm}}
                \toprule
                Approach                                       & Key Characteristics                                                   & Data Source(s)              & Semantic Capability & Strengths                                                                                  & Limitations                                                                                                                   \\
                \midrule
                Manual Review                                  & Human experts (advisors, faculty) compare syllabi descriptions.       & Course Catalogs, Syllabi    & High (Human-level)  & Nuanced, context-aware, trusted by faculty.                                                & Extremely slow, not scalable, subjective, prone to inconsistency.                                                             \\
                Keyword/ TF-IDF                                & Bag-of-words representation, statistical term weighting.              & Course Catalogs             & None to Low         & Simple, computationally cheap, easy to implement.                                          & Fails to capture synonyms, context, or true semantic meaning                                                                  \\
                Static\hspace{3em}Embeddings (Word2Vec/ GloVe) & Pre-trained word vectors, often averaged for document representation. & Course Catalogs             & Medium              & Captures word-level semantics, better than TF-IDF.                                         & Context-insensitive, averaging vectors loses information.                                                                     \\
                Enrollment-Based (e.g., course2vec)            & Embeddings learned from student co-enrollment patterns.               & Proprietary Student Records & High (Behavioral)   & Captures functional relationships between courses, highly predictive.                      & Requires access to sensitive private data, not generalizable, privacy concerns.                                               \\
                Direct LLM Classification                      & End-to-end classification using prompt engineering.                   & Course Catalogs             & Very High           & High accuracy potential, understands complex language.                                     & Computationally expensive, ``black box'' opacity, prompt sensitive, no quantifiable similarity score, risk of hallucinations. \\
                Proposed Method (Embeddings + ML)              & Deep contextual embeddings as features for traditional classifiers.   & Course Catalogs             & Very High           & State-of-the-art accuracy, computationally efficient, quantifiable, uses public data only. & Relies on the quality of the pre-trained embedding model.                                                                     \\
                \bottomrule
                % \multicolumn{4}{p{6cm}}{\scriptsize $^{*}$ Huggingface Overall Leaderboard Rank}      \\
                % \multicolumn{4}{p{6cm}}{\scriptsize $^{\dagger}$ in Millions}
            \end{tabular}
        }
    \end{table}

\section{Embedding-Based Classification}\label{sec:embclass}
In response to the limitations of existing methods, this thesis proposes and validates a novel framework for automating course equivalency determination. This approach is designed to achieve state-of-the-art accuracy while remaining computationally efficient, transparent, and reliant only on publicly available data. It achieves this by strategically decoupling the process of semantic understanding from the final classification task, leveraging the distinct strengths of deep embedding models and traditional machine learning classifiers.

\subsection{Feature Engineering Engine}
The foundational principle of the proposed framework is to utilize state-of-the-art deep embedding models not as end-to-end classifiers, but as highly sophisticated feature extraction engines. In this paradigm, a publicly available course description, in its raw text form, is processed by a pre-trained contextual embedding model. The model's task is to convert the unstructured text into a high-dimensional numerical vector that encapsulates the rich semantic content of the course.

This computation is performed once for each course in the catalog, transforming the entire corpus of unstructured text into a structured database of semantic vectors. This pre-processing step creates a reusable and efficient representation of each course, directly addressing the primary drawbacks of the direct LLM classification approach. By storing these embeddings, the framework eliminates the need to repeatedly process the same raw text through a costly API, drastically reducing computational overhead and latency for subsequent comparisons. This makes the system inherently more scalable and suitable for real-world applications that may involve hundreds of thousands of courses.

\subsection{Distance Metric for Classification}
A central innovation of this research lies in the method used to compare two course embeddings. Preliminary analysis revealed that relying on a single, holistic similarity metric like cosine similarity was insufficient for establishing a clear and reliable decision boundary between equivalent and non-equivalent course pairs. To overcome this, a composite distance vector was developed to serve as a richer feature set for the classification model.

This feature vector, denoted as \(\Delta_c\), is constructed by concatenating the element-wise difference of the two course embedding vectors (\(\mathbf{A}\) and \(\mathbf{B}\)) with their cosine similarity. The formal definition is as follows:
\[ \Delta_c = \left(a_1 - b_1, \dots, a_k - b_k, \frac{\mathbf{A}\cdot\mathbf{B}}{\parallel \mathbf{A} \parallel \parallel \mathbf{B} \parallel } \right) \]
where \(\mathbf{A} = (a_1, \dots, a_k) \) and \(\mathbf{B} = (b_1, \dots, b_k) \) are the \(k\)-dimensional embedding vectors for the two courses.  This design is powerful because it provides the subsequent classifier with two distinct types of information simultaneously. The element-wise difference captures granular, dimension-specific (local) disparities between the two semantic representations, while the cosine similarity provides a single, normalized measure of their overall (global) alignment in the vector space. This composite vector creates a much more informative and discriminative feature set than a single similarity score alone.

\subsection{Classical Machine Learning}
By transforming the comparison of two courses into the generation of a fixed-length feature vector (\(\Delta_c\)), the complex problem of semantic evaluation is effectively reduced to a standard, supervised classification task. This allows for the application of a suite of well-understood and highly optimized traditional machine learning algorithms. In this research, models including Logistic Regression, Support Vector Machines (SVM), k-Nearest Neighbors (k-NN), and Random Forests (RF) were trained on these generated distance vectors to predict a binary label: equivalent or not equivalent.

This hybrid approach demonstrates remarkable efficacy. Experimental results show that the non-linear models, in particular, achieve exceptional performance. Across various embedding models and dimensionality reduction techniques, models such as k-NN, SVM, and RF consistently produced \(F_1\)-scores ranging from 0.971 to an exceptionally high 0.996. This level of accuracy represents a significant improvement over previously reported methods and may achieve the level of reliability required for a production system.

Furthermore, this framework proves to be highly efficient. A key finding is that smaller, more computationally modest fine-tuned embedding models (e.g.\ the 33-million-parameter BGE model) can achieve performance that is comparable to, and in some cases better than, general pre-trained models that are orders of magnitude larger and more complex (e.g.\ the 7.8-billion-parameter NV-EmbedV2 a model) when used within this framework.  This has profound practical implications, suggesting that state-of-the-art results can be achieved without requiring massive computational resources or reliance on proprietary, closed-source models. Finally, this approach offers a degree of interpretability that is absent in end-to-end LLM solutions. Models like Random Forest can provide feature importance metrics, offering insights into which dimensions of the semantic space are most critical for determining equivalency, thereby opening the ``black box'' of the decision-making process.

\section{Contributions}
This research makes several key contributions to the fields of educational data mining and natural language processing, offering a practical and powerful solution to the long-standing challenge of course articulation.

\subsection{Primary Contributions}
\begin{enumerate}
\item \textbf{A High-Accuracy, Automated Framework}: This thesis develops and validates a novel framework for determining course equivalency that achieves state-of-the-art accuracy, with \(F_1\)-scores exceeding 0.99 on a challenging real-world dataset. Crucially, it accomplishes this using only publicly available course catalog text, making it broadly applicable.
\item \textbf{An Innovative Feature Engineering Technique}: It introduces a composite distance vector, \(\Delta_c\), that uniquely combines element-wise embedding differences with cosine similarity. This technique provides a richer input signal for classification and is shown to demonstrably improve the performance of downstream machine learning models, particularly linear classifiers.
\item \textbf{A Computationally Efficient and Scalable Approach}: The research demonstrates that by decoupling semantic representation from classification, it is possible to harness the power of deep contextual embeddings without the high computational costs, API dependencies, and opaque nature of direct LLM-based classification. This makes the proposed solution more efficient, scalable, and practical for institutional deployment.
\item \textbf{A Privacy-Preserving Methodology}: By relying exclusively on public course descriptions, the proposed method circumvents the significant privacy, security, and data access challenges associated with techniques that require sensitive student enrollment records. This makes the framework more ethically sound and generalizable across any pair of institutions, regardless of their data-sharing agreements.
\end{enumerate}

\subsection{Thesis Roadmap}
The remainder of this thesis is structured to provide a comprehensive account of this research.
\begin{enumerate}
\item \textbf{Chapter 1: Background and Related Work} will provide a detailed expansion of the topics covered in Sections~\ref{sec:automation} and~\ref{sec:embclass}, offering an in-depth survey of the student transfer landscape and the evolution of technological interventions.
\item \textbf{Chapter 2: Methodology} will offer a deep dive into the data collection and preparation processes, the specific embedding models evaluated, the construction of the feature vectors, and the theoretical underpinnings of the machine learning classifiers employed.
\item \textbf{Chapter 3: Experimental Setup and Results} will detail the experimental design, the datasets used for training and validation, and a comprehensive analysis of the classification performance, including ablation studies and model comparisons.
\item \textbf{Chapter 4: Discussion and Future Work} will interpret the results in a broader context, discuss the limitations of the current study, and outline promising avenues for future research, including the development of a full-scale course recommendation system and the exploration of fine-tuning techniques.
\item \textbf{Chapter 5: Conclusion} will summarize the key findings of the thesis and reiterate the significance of its contributions to both academic research and the practical administration of higher education.
\end{enumerate}

\end{introduction}
