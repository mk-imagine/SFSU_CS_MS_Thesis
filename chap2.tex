\chapter{Background and Related Works}

The manifest inefficiencies and inequities of manual course articulation, exemplified by the challenges within California's vast system, have prompted a range of research efforts aimed at automating the process~\cite{ma_course_recommendation_2017, PardosCourse2Vec2019, pardos-articulation-2019, JiangPardosMulti2VecEDM2020,XuPardosSubwordEmbeddings2024}. These technological interventions have evolved in sophistication, mirroring the broader advancements in natural language processing (NLP) and machine learning. A critical review of this literature reveals a clear trajectory from simple statistical methods to complex deep learning models, with each stage introducing new capabilities while also exposing new limitations. This evolution illuminates the path toward a more robust and scalable solution.

\section{Keyword and Statistical Methods}
The earliest attempts at automating course comparison relied on foundational text analysis techniques that, while computationally simple, lack semantic depth. The most basic systems are essentially search engines or databases that depend on exact keyword matching or pre-populated tables of known equivalencies~\cite{shamrock}. These systems are inherently brittle; they cannot recognize semantic variations (e.g., equating ``Introduction to Programming'' with ``Fundamentals of Computer Science I'') and require continuous manual updates to remain relevant~\cite{shiferaw2024}.

A more advanced statistical method, Term Frequency-Inverse Document Frequency (TF-IDF), improves upon keyword matching by vectorizing documents and weighting terms based on their importance. A term's frequency within a single document (TF) is balanced against its rarity across a collection of documents, or corpus (IDF)~\cite{AIZAWA200345}. This allows the model to assign higher importance to distinctive terms (e.g., ``calculus'') and lower importance to common words (e.g., ``the,'' ``a,'' ``is'')~\cite{AIZAWA200345}. TF-IDF has been a workhorse for information retrieval and has been applied to course similarity tasks~\cite{AIZAWA200345}. However, the fundamental limitation of TF-IDF and other bag-of-words models is their complete lack of semantic understanding. They treat words as discrete, unrelated tokens and cannot grasp that ``calculus'' and ``differentiation'' are related concepts, nor can they distinguish between different meanings of the same word.

\section{Static Semantic Representations}
The development of word embeddings represented the first major leap toward a true semantic understanding of course content. Models like Word2Vec and GloVe, trained on vast text corpora, learn to represent words as dense vectors in a high-dimensional space, where words with similar meanings are positioned closer to one another. For example, the vectors for ``car'' and ``automobile'' would be near each other, while being distant from the vector for ``planet.'' This innovation enabled a more nuanced comparison of texts than was possible with TF-IDF. In the context of educational data mining, these techniques have been applied to content-based course recommendation systems, typically by creating a single vector representation for a course description by averaging the vectors of all its constituent words~\cite{pardos10.1145/3330430.3333622}.

Despite this advancement, these models produce static embeddings. Each word is assigned a single, fixed vector regardless of its context. This is a significant drawback, as it fails to account for polysemy: words with multiple meanings. For instance, the word ``bank'' would have the same vector in the phrases ``river bank'' and ``bank account,'' despite their disparate meanings. Furthermore, the common practice of averaging all word vectors to create a document-level representation is a crude heuristic that can dilute or lose critical semantic information, especially in complex or lengthy descriptions.

\section{Contextual Semantic Representations}
The introduction of the transformer architecture, and specifically models like Bidirectional Encoder Representations from Transformers (BERT), revolutionized NLP by enabling the generation of contextual embeddings. In these models, the vector representation of a word is dynamically influenced by the words surrounding it in a sentence~\cite{devlin2019bertpretrainingdeepbidirectional}. This allows the model to disambiguate word meanings and capture a much richer, more accurate semantic representation of the text. Architectures such as Sentence-BERT (SBERT) were subsequently developed to fine-tune these models specifically for the task of producing semantically meaningful embeddings for entire sentences or short paragraphs, which can then be efficiently compared using metrics like cosine similarity~\cite{reimers-2019-sentence-bert}.

 %%%%% NEED TO FLESH OUT %%%%%%%%%
\section{Direct LLM Classification}
A more recent evolution in this domain involves the direct application of large-scale generative models, or Large Language Models (LLMs) like GPT-4 and Gemini, for classification tasks. The preliminary work for this thesis explored this very approach. Through sophisticated prompt engineering and in-context learning, these models can be instructed to perform pairwise comparisons of course descriptions and render a judgment on their equivalency.  

While these experiments yielded promising accuracy, they also uncovered significant practical and theoretical limitations. The direct use of LLMs for this task is computationally expensive and inefficient, as it requires repeatedly sending full text descriptions to a model API for every comparison. Performance is acutely sensitive to the exact phrasing of the prompt, necessitating a costly and time-consuming iterative tuning process. Most critically, the decision-making process of an LLM is a ``black box,'' providing a categorical output (e.g. ``equivalent'') without a quantifiable similarity score or confidence level. This opacity makes it difficult to rank potential matches, set decision thresholds, or provide transparent justifications for the model's conclusions. This approach is also ill-suited for handling more complex articulation scenarios, such as one-to-many or many-to-many course mappings.  A detailed review of our efforts are outlined in Chapter~\ref{ch:3.1}.

\section{Enrollment-Based Approaches}
It is essential to situate this research within the context of parallel efforts that leverage different data sources. A notable body of work has demonstrated that course similarity and prerequisite relationships can be predicted with high accuracy by analyzing student enrollment data. Models such as \emph{course2vec} learn course embeddings not from their textual descriptions, but from the patterns of which courses students tend to take together~\cite{pardos2018connectionistrecommendationwildutility}. The underlying principle is that courses frequently taken in the same semester or in sequence likely share a functional or topical relationship~\cite{pardos2018connectionistrecommendationwildutility}.

While this behavioral approach is powerful, its reliance on large-scale, proprietary institutional datasets of student records presents two major obstacles. First, it raises significant data privacy and security concerns, as it involves the analysis of sensitive student information~\cite{slade10.1177/0002764213479366}. Second, it limits the scalability and generalizability of the solution. The model is only applicable at institutions that can provide access to such data, and it cannot be used to compare courses between two institutions that have no history of student transfer between them. This approach is therefore not a universal solution for the broader course articulation problem.

The evolution of these varied approaches reveals a fundamental trade-off: as models gain greater semantic power, they tend to become more computationally intensive, less interpretable, and more demanding of specialized or private data. The limitations of direct LLM classification (cost, opacity) and enrollment-based methods (data privacy, limited access) point toward the need for a new paradigm. An ideal solution should harness the semantic power of large pre-trained models without inheriting their operational burdens. This suggests that the next logical step is not simply a larger, more complex end-to-end model, but rather a more intelligent, hybrid framework. Such a framework would decouple the task of deep semantic representation from the task of final classification, allowing each component to be optimized for what it does best. This conceptual shift forms the central motivation for the methodology proposed in this thesis.  Table~\ref{tbl:taxonomy} summarizes the primary methods for determining course transferability.

\begin{table}[!htb]
    \centering
    \setlength{\tabcolsep}{10pt} % Default value: 6pt
    \renewcommand{\arraystretch}{1.5}  % Provide more space between table rows, if you prefer
    \caption{Comparative Taxonomy of Course Equivalency Determination Methods}\label{tbl:taxonomy}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{>{\raggedright\arraybackslash}p{2.25cm}>{\raggedright\arraybackslash}p{3cm}>{\raggedright\arraybackslash}p{1.75cm}>{\raggedright\arraybackslash}p{1.9cm}>{\raggedright\arraybackslash}p{3.75cm}>{\raggedright\arraybackslash}p{3.75cm}}
            \toprule
            Approach                                       & Key Characteristics                                                   & Data Source(s)              & Semantic Capability & Strengths                                                                                  & Limitations                                                                                                                   \\
            \midrule
            Manual Review                                  & Human experts (advisors, faculty) compare syllabi descriptions.       & Course Catalogs, Syllabi    & High (Human-level)  & Nuanced, context-aware, trusted by faculty.                                                & Extremely slow, not scalable, subjective, prone to inconsistency.                                                             \\
            Keyword/ TF-IDF                                & Bag-of-words representation, statistical term weighting.              & Course Catalogs             & None to Low         & Simple, computationally cheap, easy to implement.                                          & Fails to capture synonyms, context, or true semantic meaning                                                                  \\
            Static\hspace{3em}Embeddings (Word2Vec/ GloVe) & Pre-trained word vectors, often averaged for document representation. & Course Catalogs             & Medium              & Captures word-level semantics, better than TF-IDF.                                         & Context-insensitive, averaging vectors loses information.                                                                     \\
            Enrollment-Based (e.g., course2vec)            & Embeddings learned from student co-enrollment patterns.               & Proprietary Student Records & High (Behavioral)   & Captures functional relationships between courses, highly predictive.                      & Requires access to sensitive private data, not generalizable, privacy concerns.                                               \\
            Direct LLM Classification                      & End-to-end classification using prompt engineering.                   & Course Catalogs             & Very High           & High accuracy potential, understands complex language.                                     & Computationally expensive, ``black box'' opacity, prompt sensitive, no quantifiable similarity score, risk of hallucinations. \\
            Proposed Method (Embeddings + ML)              & Deep contextual embeddings as features for traditional classifiers.   & Course Catalogs             & Very High           & State-of-the-art accuracy, computationally efficient, quantifiable, uses public data only. & Relies on the quality of the pre-trained embedding model.                                                                     \\
            \bottomrule
            % \multicolumn{4}{p{6cm}}{\scriptsize $^{*}$ Huggingface Overall Leaderboard Rank}      \\
            % \multicolumn{4}{p{6cm}}{\scriptsize $^{\dagger}$ in Millions}
        \end{tabular}
    }
\end{table}