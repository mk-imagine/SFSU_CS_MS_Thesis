\chapter{Methodology}
In response to the limitations of existing methods, this thesis proposes and validates a novel framework for automating course equivalency determination. This approach is designed to achieve state-of-the-art accuracy while remaining computationally efficient, transparent, and reliant only on publicly available data. It achieves this by strategically decoupling the process of semantic understanding from the final classification task, leveraging the distinct strengths of deep embedding models and traditional machine learning classifiers.

The foundational principle of the proposed framework is to utilize state-of-the-art deep embedding models not as end-to-end classifiers, but as highly sophisticated feature extraction engines. In this paradigm, a publicly available course description, in its raw text form, is processed by a pre-trained contextual embedding model. The model's task is to convert the unstructured text into a high-dimensional numerical vector that encapsulates the rich semantic content of the course.

This computation is performed once for each course in the catalog, transforming the entire corpus of unstructured text into a structured database of semantic vectors. This pre-processing step creates a reusable and efficient representation of each course, directly addressing the primary drawbacks of the direct LLM classification approach. By storing these embeddings, the framework eliminates the need to repeatedly process the same raw text through a costly API, drastically reducing computational overhead and latency for subsequent comparisons. This makes the system inherently more scalable and suitable for real-world applications that may involve hundreds of thousands of courses.

This chapter delineates the comprehensive and systematic methodology employed to develop and evaluate a sophisticated classification pipeline. The central objective of this research is to construct a robust predictive model capable of accurately classifying domain-specific textual data. The methodological approach is predicated in a multi-stage process, designed to address the inherent complexities of natural language and to maximize predictive performance. This process begins with the acquisition and preparation of the data corpus, proceeds to the creation of semantically rich numerical representations through deep metric learning---a process that uses a deep neural network to learn an embedding space where the distance (or metric) between similar items is minimized and the distance between dissimilar items is maximized---and culminates in the systematic training and evaluation of a diverse suite of machine learning classifiers.

The narrative of this chapter follows the logical and sequential flow of the research execution. First, it details the data acquisition, pre-processing, and structuring protocols, with a particular focus on the preparation of data for advanced metric learning techniques. Second, it provides a deep dive into the core of the feature engineering strategy: the fine-tuning of a pre-trained transformer model using a triplet loss function. This section offers a rigorous examination of the underlying architecture, the theoretical principles of the chosen loss function, and the rationale behind the selection of optimizer and learning rate scheduling framework. Third, the chapter describes the process of generating the feature vectors from fine-tuned models, effectively transforming the unstructured text problem into a structured format suitable for classical machine learning. Fourth, it presents the theoretical foundations and application of each downstream classification algorithm evaluated in this study, justifying their inclusion based on their distinct learning paradigms. Finally, the chapter outlines the dual evaluation framework used to select the optimal embedding model during the fine-tuning phase, and assess the performance of the final classifiers on a held-out test set. This structured and principled methodology ensures the rigor, reproducibility, and validity of the research findings presented in subsequent chapters.

\section{Data Corpus and Pre-processing}
The foundation of any machine learning endeavor is the data upon which it is trained and evaluated. This section describes the characteristics of the dataset used in this study, the pre-processing pipeline applied to ensure data quality and consistency, and the specific data structuring techniques required to facilitate the deep metric learning phase of the research.

\subsection{Dataset Characterization}
The primary data for this research was sourced from an expanded dataset provided in partnership with Program Pathways Mapper (PPM), initially consisting of 2217 labeled courses. The class label for each course is its Course Identification Numbering System (C-ID) code, which serves as a ground truth for equivalency.

A crucial pre-processing step was performed to ensure that sufficient examples existed for robust training and evaluation. To guarantee that both the training and test sets would contain at least one pair of equivalent courses for every class, all classes with fewer than four courses in the total dataset were removed. This filtering process resulted in a final corpus of 2157 courses distributed across 157 distinct C-ID classes.

To ensure a rigorous and unbiased evaluation of the final models, this final dataset was partitioned into two distinct, non-overlapping subsets: a training set and a test set. A stratified 50/50 split was employed, using the C-ID code as the stratification key. This resulted in a training set of 1078 courses and a test set of 1079 courses. The stratification ensures that each of the 157 classes is represented in both subsets, with each class containing between 2 and 33 equivalent courses per set.  The test set is held in reserve and used only once for the final, conclusive evaluation of the optimized classification pipeline, providing an honest estimate of the model's generalization performance on unseen data.

The training set serves a dual purpose in this methodology, a strategy made possible by the different data formats required by the training and validation procedures. For the fine-tuning process itself, the training set is used to generate (anchor, positive, negative) triplets as required by the Triplet Loss functions. For the validation within the fine-tuning process, this same training set is utilized to generate binary pairs of ``equivalent'' and ``non-equivalent'' course pairs. This validation set is then used with the BinaryClassificationEvaluator to monitor model performance during training and select the best model checkpoint. This approach allows for robust validation without data leakage, as the model is evaluated on our primary task (binary pair classification) than the one it is directly optimizing for (triplet distance minimization).

\subsection{Document Normalization for Embedding}
In a departure from conventional NLP pipelines, this research deliberately eschewed standard text pre-processing techniques such as lowercasing, stop-word removal, or the stripping of special characters. This decision was made to more accurately simulate a real-world use case where input data, such as course information copied directly from a university website, may be imperfectly formatted or contain extraneous characters. The methodology, therefore, relies on the inherent semantic power and robustness of modern transformer-based embedding models to interpret and handle this "raw" text.

Instead of cleaning the text, a normalization step was performed to create a consistent, structured input for the embedding models. A new field, ``Formatted Course Info,'' was generated for each course by concatenating four key pieces of information: the department name, the department course number, the course title, and the full course description. This concatenated string serves as the single document representation for each course and is the direct input for the document embedding process described in Section %%%%. This approach ensures that all relevant textual context is preserved in a standardized format before being converted into a numerical vector.

\subsection{Data Structuring for Fine-Tuning}\label{ss:datastructure}
The family of triplet loss functions operates on a conceptual structure of (anchor, positive, negative) triplets~\cite{reimers-2019-sentence-bert,sbertLossOverview}. However, a key advantage of the batch-based triplet loss functions used in this research is that they do not require the manual, pre-generation of these triplets. Instead, the model is trained on a dataset of (text, label) pairs~\cite{sbertLossOverview}. The loss function then intelligently forms the necessary triplets on-the-fly from within each mini-batch of data. This online mining approach is significantly more efficient and effective than offline strategies. These triplets are defined as:
\begin{itemize}
    \item \textbf{Anchor (A)}: A reference sentence or text sample.
    \item \textbf{Positive (P)}: A sentence that belongs to the same class as the anchor.
    \item \textbf{Negative (N)}: A sentence that belongs to a different class from the anchor.
\end{itemize}
The training process involves feeding these triplets to the model and optimizing it to minimize the distance between the anchor and positive embeddings while maximizing the distance between the anchor and negative embeddings in the vector space.

The generation of these triplets from the normalized, labeled training set is a critical preparatory step. A crucial constraint imposed by triplet loss functions, particularly those available in the sentence-transformers library, is that the training data must contain a minimum of two examples for each class label~\cite{sbertLosses}. This is not a superficial requirement but a fundamental necessity for the loss computation to be valid. The formation of an (anchor, positive) pair requires selecting two distinct samples from the same class~\cite{sbertLosses}. If a class is represented by only one sample, it can never form a positive pair and can only contribute to (anchor, negative) pairings.

This constraint has a direct and significant implication for the data loading and batching strategy. A simple random sampling approach could easily result in mini-batches that lack the necessary class diversity, containing only one or even zero instances of certain classes. In such a scenario, no valid triplets could be formed for the anchors of those underrepresented classes within that batch, leading to an inefficient and potentially biased training process. To mitigate this, a more sophisticated data loading mechanism was required. Rather than implementing a fully custom solution, this research leveraged a specialized, built-in feature of the sentence-transformers library: the \verb|GroupByLabelBatchSampler|. This sampler, activated by setting the \verb|batch_sampler| argument to \verb|BatchSamplers.GROUP_BY_LABEL| during trainer configuration, is specifically designed for use with triplet loss functions~\cite{sbertSamplers}. It addresses the sampling challenge by ensuring that each mini-batch is constructed by grouping samples with the same label, thereby guaranteeing that every batch contains the necessary class diversity to form valid and informative triplets for all anchors. This demonstrates a key principle of the methodology: the data preparation and loading stage is not an independent preliminary step but is intrinsically linked to and constrained by the fine-tuning objective.

\section{Embedding Model Selection and Fine-Tuning}
A central hypothesis of this research is that a generic, pre-trained language model can be adapted to produce highly specialized and semantically rich embeddings for the specific domain of the data corpus. These bespoke embeddings are expected to provide a more discriminative feature representation for downstream classification tasks compared to off-the-shelf embeddings. This section details the architecture, learning objective, and training protocol used to achieve this adaptation through a process of deep metric learning.

\subsection{Selection}
The selection of an appropriate embedding model is a critical first step that influences the entire downstream pipeline. Rather than selecting a single model, this research began with a broad preliminary analysis of a variety of open-source embedding models to identify strong candidates for a more in-depth, comparative study. The initial models reviewed spanned a wide range of parameter sizes and characteristics which are summarized in Table~\ref{tbl:emb_models}.
\begin{table}[!htb]
    \centering
    % \renewcommand{\arraystretch}{1.2}  % Provide more space between table rows, if you prefer
    \caption{Initial Embedding Model Review}
    \label{tbl:emb_models}
    % \resizebox{\columnwidth}{!}{
    \begin{tabular}{lcccc}
        \toprule
        Model Name                 & Rank$^{*}$ & Params$^{\dagger}$ & Dims & Acc             \\
        \midrule
        GIST-small-Embedding-v0    & 41         & 33                 & 384  & 0.9759          \\
        bge-small-en-v1.5          & 47         & 33                 & 384  & 0.9670          \\
        GIST-Embedding-v0          & 33         & 109                & 768  & 0.9768          \\
        bge-base-en-v1.5           & 35         & 109                & 768  & 0.9732          \\
        gte-base-en-v1.5           & 31         & 137                & 768  & 0.9732          \\
        mxbai-embed-large-v1       & 24         & 335                & 1024 & 0.9759          \\
        gte-large-en-v1.5          & 21         & 434                & 1024 & 0.9777          \\
        multilingual-e5-large-inst & 34         & 560                & 514  & 0.9670          \\
        stella\_en\_1.5B\_v5       & 3          & 1543               & 8192 & 0.9857          \\
        SFR-Embedding-2\_R         & 4          & 7111               & 4096 & \textbf{0.9839} \\
        Agte-Qwen2-7B-instruct     & 5          & 7613               & 3584 & 0.9804          \\
        nvidia/NV-Embed-v2         & 1          & 7851               & 4096 & 0.9831          \\
        \bottomrule
        \multicolumn{4}{p{6cm}}{\scriptsize $^{*}$ Huggingface Overall Leaderboard Rank}      \\
        \multicolumn{4}{p{6cm}}{\scriptsize $^{\dagger}$ in Millions}
    \end{tabular}
    % }
\end{table}

This preliminary evaluation was conducted using a simple accuracy metric: for a given anchor course, a model was considered correct if the cosine similarity to an equivalent course was greater than the similarity to a non-equivalent course. This initial screening revealed that while performance varied, many models achieved high accuracy, with a sample mean of \(0.9767\) and a standard deviation of \(0.00605\). Based on these results and a desire to evaluate a representative spectrum of model sizes, three models were selected for the primary analysis:
\begin{itemize}
    \item \verb|BAAI/bge-small-en-v1.5| (BGE): Representing a high-performing small model (33M parameters).
    \item \verb|avsolatorio/GIST-Embedding-v0| (GIST): Representing a medium-sized model (109M parameters).
    \item \verb|nvidia/NV-Embed-v2| (NVE): Representing a large-scale model (7.85B parameters).
\end{itemize}

At a later stage of the research, \verb|Salesforce/SFR-Embedding-2_R| (SFR) was also included for additional comparison due to its strong performance on public leaderboards. The foundational architecture for all these models is the transformer, which allows them to generate rich, contextual embeddings. By employing a pooling layer (typically mean-pooling), these models produce a single, fixed-size vector for each input document, making them highly suitable for feature extraction.

\subsection{Metric Learning with Batch Triplet Loss Functions}
To determine if performance could be further improved, two of the selected models, BGE and GIST, were subjected to a fine-tuning process using a metric learning approach. Metric learning aims to learn an embedding space where the geometric distance between samples corresponds to their semantic similarity.

\subsubsection{Theoretical Foundation of Triplet Loss}
The concept of Triplet Loss was first introduced by Yu, et al. in the context of face recognition and has since been widely applied to supervised similarity learning~\cite{Yu2020}. It operates on the (Anchor, Positive, Negative) triplets described in Section~\ref{ss:datastructure}. The fundamental goal of Triplet Loss is to train the embedding function, \(f(x)\), to map inputs into a vector space where the distance between an anchor sample (\(A\)) and a positive sample (\(P\)) from the same class is smaller than the distance between the anchor and a negative sample (\(N\)) from a different class. To prevent the model from collapsing all embeddings to a single point and to ensure a meaningful separation, the loss function enforces a margin, \(\alpha\). The distance between the anchor-positive pair must be smaller than the distance to the anchor-negative pair by at least this margin.

The mathematical formulation of the Triplet Loss is given by:
\[ L(A,P,N)=\max(d(f(A),f(P))-d(f(A),f(N))+\alpha,0). \]
Here, \(d\) represents a distance metric, which in this study is the Euclidean distance. The \(\max(d(f(A),f(P))-d(f(A),f(N))+\alpha,0)\) component ensures that the loss is only incurred for triplets that violate the margin constraint. If the negative sample is already further from the anchor than the positive sample by at least the margin, the loss for that triplet is zero, and no weight update is performed.

\subsubsection{Online Triplet Mining and Batch-based Losses}
A naive implementation of Triplet Loss would form all possible triplets from the training data, an approach known as "offline" mining. This is computationally infeasible for large datasets and highly inefficient, as the vast majority of triplets are "easy" and provide no useful learning signal. A more effective approach is "online" mining, where informative triplets are selected on-the-fly from within each mini-batch of training data. Within this paradigm, triplets can be categorized by their difficulty :
\begin{itemize}
    \item \textbf{Easy Triplets}: These triplets already satisfy the margin constraint, i.e., \(d(A,P)+\alpha<d(A,N)\). They result in zero loss and do not contribute to learning.
    \item \textbf{Semi-Hard Triplets}: In these triplets, the negative sample is more distant than the positive sample but still lies within the margin, i.e., \(d(A,P)<d(A,N)<d(A,P)+\alpha\). These triplets have a positive loss and provide a useful, stable gradient for training.
    \item \textbf{Hard Triplets}: These are the most challenging cases, where the negative sample is closer to the anchor than the positive sample, i.e., \(d(A,N)<d(A,P)\). These triplets produce the largest loss values and provide the strongest learning signal, forcing the model to learn fine-grained distinctions.
\end{itemize}

\subsubsection{Empirical Evaluation of Batch Triplet Loss Functions}
Recognizing that different triplet mining strategies can significantly impact model performance, this research empirically evaluated all four primary batch-based triplet loss implementations available in the sentence-transformers library to identify the optimal strategy for this specific dataset and task. This approach ensures that the chosen loss function is best suited to the data's characteristics, rather than relying on a single, pre-selected method. The evaluated loss functions were:
\begin{itemize}
    \item \textbf{BatchAllTripletLoss}: This function computes the loss for all valid triplets that can be formed within a given batch. While this approach is comprehensive, it can be computationally intensive, and the learning signal can be diluted by the high number of "easy" triplets that contribute zero to the loss.
    \item \textbf{BatchSemiHardTripletLoss}: This function focuses the training effort by considering only semi-hard triplets for each anchor. This is a common strategy that provides stable training but can sometimes lead to slower convergence as it ignores the most challenging "hard" examples that often provide the strongest learning signal.
    \item \textbf{BatchHardTripletLoss}: This function implements a more aggressive mining strategy. For each anchor sample in a mini-batch, it identifies the hardest positive (the positive sample that is furthest away) and the hardest negative (the negative sample that is closest). The loss is then computed on these most challenging pairs. This can accelerate convergence but is also known to be ``temperamental,'' potentially leading to a noisy and difficult optimization landscape.
    \item \textbf{BatchHardSoftMarginTripletLoss}: This function is a variation of BatchHardTripletLoss that also focuses on the hardest positive and negative samples but does not require a manually specified margin parameter. This simplifies hyperparameter tuning while still leveraging an aggressive mining strategy.
\end{itemize}
The final model used for feature generation was trained with the loss function that demonstrated the best performance on the validation set during this empirical evaluation.

\subsection{Optimization and Training Protocol}
The successful fine-tuning of a model with a challenging objective like \verb|BatchHardTripletLoss| is critically dependent on the choice of optimizer and learning rate schedule. The components selected for this research, AdamW and Cosine Annealing with Warm Restarts, were not chosen in isolation but as parts of a cohesive, synergistic framework designed to ensure stable and effective learning.

\subsubsection{Optimizer Selection: AdamW}
The optimization of the model's weights was performed using the AdamW optimizer. The foundation of AdamW is the Adam (Adaptive Moment Estimation) optimizer, which has become a de facto standard in deep learning due to its computational efficiency and effectiveness. Adam combines the benefits of two other optimization algorithms: the momentum method, which helps accelerate gradient descent in the relevant direction, and RMSprop, which adapts the learning rate for each parameter based on the magnitude of past gradients. This per-parameter adaptive learning rate makes Adam particularly well-suited for the noisy gradients that can arise from \verb|BatchHardTripletLoss|.

However, the standard implementation of Adam has a subtle flaw in how it handles L2 regularization, also known as weight decay. In standard Adam, the weight decay term is coupled with the gradient update itself. This means that the regularization effect is influenced by the magnitude of the gradient, leading to an inconsistent application of weight decay. For parameters with large gradients, the effective weight decay is reduced, which can hinder proper regularization and lead to poorer generalization~\cite{loshchilov2019decoupledweightdecayregularization}.

AdamW, proposed by Loshchilov and Hutter, corrects this flaw by decoupling the weight decay from the gradient update step. The adaptive moment updates are performed as in standard Adam, but the weight decay is applied as a separate, final step, directly modifying the weights. This decoupled approach ensures that weight decay acts as a true regularizer, penalizing large weights uniformly without interfering with the optimizer's adaptive learning mechanism. For a complex and potentially unstable training process like the one employed here, the enhanced generalization and training stability offered by AdamW make it a superior choice over standard Adam~\cite{loshchilov2019decoupledweightdecayregularization}. It provides the necessary regularization to prevent overfitting on the ``hard'' examples mined by the loss function, while preserving the adaptive learning that helps navigate the complex loss surface.

\subsubsection{Learning Rate Schedule: Cosine Annealing with Warm Restarts}
Static or linear learning rates are often suboptimal for training deep neural networks. A global learning rate schedule, which dynamically adjusts the learning rate during training, can significantly improve convergence and final model performance, even when using an adaptive gradient algorithm such as AdamW~\cite{loshchilov2019decoupledweightdecayregularization}. For this research, the \verb|CosineAnnealingWarmRestarts| schedule was employed. This schedule combines two powerful concepts: cosine annealing and warm restarts. Cosine annealing smoothly decays the learning rate from an initial maximum value, \(\nu_{\textrm{max}}\), to a minimum value, \(\nu_{\textrm{min}}\), following the shape of a cosine curve. The learning rate \(\nu_t\) at a given epoch \(T_{\textrm{cur}}\) within a cycle of length \(T_i\) is calculated as:
\[ \nu_t = \nu_{\textrm{min}} + \frac{1}{2}\left( \nu_{\textrm{max}} -\nu_{\textrm{min}} \right)\left(1 + \cos\left(\frac{T_{\textrm{cur}}}{T_{\textrm{max}}}\pi\right)\right) \]
This smooth, gradual decay is often more effective than abrupt step-wise decays, allowing the model to fine-tune its parameters more carefully as it approaches a minimum in the loss landscape~\cite{pytorchcosanneal}.  The warm restarts technique introduces periodic ``restarts'' into the training process. After a specified number of epochs, \(T_0\), the learning rate is abruptly reset to its initial maximum value, \(\nu_{\textrm{max}}\), and the cosine decay cycle begins anew. The length of subsequent cycles can be progressively increased by a multiplicative factor, \(T_{\textrm{mult}}\).  For example, if \(T_0 = 10\) and \(T_{\textrm{mult}}=2\), the restarts would occur after 10, 30 (10+20), 70 (10+20+40), and so on, epochs.

The combination of these two techniques forms a powerful optimization strategy. The aggressive nature of \verb|BatchHardTripletLoss| creates a complex loss surface with many sharp, suboptimal local minima. A standard decay schedule might cause the optimizer to converge into one of these minima and become stuck. The ``warm restarts'' provide a crucial escape mechanism. When the model's performance on the validation set begins to plateau, suggesting convergence into a local minimum, the sudden increase in the learning rate effectively ``kicks'' the optimizer out of that basin, allowing it to explore other regions of the loss landscape. The subsequent smooth cosine decay then enables the optimizer to carefully descend into any new, potentially broader and deeper, minimum it discovers.

This entire fine-tuning framework (\verb|BatchHardTripletLoss|, \verb|AdamW|, and \verb|CosineAnnealingWarmRestarts|) represents a carefully orchestrated system. The loss function provides a powerful, albeit challenging, learning signal. The optimizer provides a stable, well-regularized mechanism for applying weight updates based on that signal. The learning rate schedule provides a global exploration strategy to guide the optimizer across the complex landscape, preventing premature convergence and increasing the likelihood of finding a high-quality solution. This synergistic design is critical for successfully training a highly discriminative embedding model under these demanding conditions.

\section{Generating Course Embeddings}
Upon completion of the model selection and fine-tuning processes, the next stage of the methodology involves using the selected models to transform the entire text corpus into structured numerical formats suitable for a comparative analysis of downstream machine learning classifiers.

This process was executed by loading each selected embedding model and operating it in inference mode. The ``Formatted Course Info'' text for each course in the training and test sets was individually fed into each model to generate a fixed length embedding vector. This procedure was performed for the models summarized in Table~\ref{tbl:emb}.
\begin{table}[!hbt]
    \footnotesize
    \caption{Embedding Models \& PCA Explained Variance}
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lcp{9em}p{5em}p{2.2em}p{2.2em}p{2.2em}}
            \toprule
                                        &           &                            &                       & \multicolumn{3}{c}{Explained Variance}               \\
                                        &           & \centering\# of Parameters & \centering Embedding  & \multicolumn{3}{c}{(\# of PCs)}                      \\
            \cmidrule{5-7} Model Name     & Type Used\(^{*}\) & \centering (in Millions)   & \centering Dimensions & 70\%                                   & 80\% & 90\% \\
            \midrule
            BAAI/bge-small-en-v1.5        & OTS \& FT & \centering 33              & \centering 384        & 28                                     & 45   & 76   \\
            avsolatorio/GIST-Embedding-v0 & OTS \& FT & \centering 109             & \centering 768        & 23                                     & 40   & 73   \\
            nvidia/NV-Embed-v2            & OTS       & \centering 7851            & \centering 4096       & 20                                     & 37   & 73   \\
            Salesforce/SFR-Embedding-2\_R  & OTS       & \centering 7111            & \centering 4096       & 20                                     & 37   & 73   \\
            \bottomrule
            \multicolumn{4}{p{6cm}}{\scriptsize \(^{*}\) OTS: Off-The-Shelf; FT: Fine-Tuned}
        \end{tabular}
    }
    \label{tbl:emb}
\end{table}

The outcome of this procedure is the creation of multiple distinct sets of numerical matrices (one for the training set and one for the test set per embedding model). In each matrix, every row corresponds to a specific text sample from the original corpus, and the columns represent the dimensions of that model's embedding space. These matrices, along with their corresponding class labels, constitute the final feature sets that serve as the direct input for the suite of classification models evaluated in the next section. This step marks the critical transition from the unstructured domain of natural language processing to the structured domain of tabular data analysis, upon which the final classification experiments are built.

\section{Feature Vector Construction}
The high-dimensional embedding vectors generated in the previous step, while semantically rich, are not the final features used for classification. To prepare the data for the downstream classifiers, a two-stage feature engineering pipeline was executed. This pipeline first applies various dimensionality reduction techniques to the embeddings and then constructs pairwise difference vectors from these reduced (and original) embeddings to explicitly represent the relationship between two courses for the equivalency classification task.

\subsection{Dimensionality Reduction}
High-dimensional data can present challenges for some machine learning algorithms, a phenomenon often referred to as the ``curse of dimensionality.'' To investigate the impact of dimensionality on classifier performance and to potentially create more robust feature sets, a systematic process of dimensionality reduction was applied to the embeddings generated by each model.  

This process was managed by a dedicated \verb|EmbeddingReducer| class, which applied several reduction techniques to the training and test sets. To ensure the integrity of the evaluation and prevent data leakage, the reducer models were fit exclusively on the training data. The same fitted models were then used to transform the test data. This methodology guarantees that no information from the test set influences the parameters of the reduction models. This step resulted in multiple versions of the training and test embedding matrices: the original high-dimensional version, and several new versions corresponding to each reduction technique applied and the number of dimensions targeted.  A PCA analysis was employed to identify the number of principal components that explained 70\%, 80\%, and 90\% of the variance for each model.  These values and a static 4 and 7 dimensions were used to generate a wide variety of vectors of varying dimensions to train with and test.

\subsection{Global and Local Distance Vector}
The ultimate goal of this research is to classify pairs of courses as either “equivalent” or “not equivalent.” This requires input features that represent the relationship between two courses, not just the characteristics of a single one. Our preliminary analysis showed that relying on a single, holistic metric like cosine similarity was insufficient for establishing a clear and reliable decision boundary. To overcome this, we designed a composite feature vector that provides a richer, more discriminative representation of a course pair's relationship.

The feature vector, denoted as \(\Delta_c\), is constructed by concatenating the element-wise difference of the two course embedding vectors (\(\mathbf{A}\) and \(\mathbf{B}\)) with their cosine similarity. The formal definition is as follows:
\[ \Delta_c = \left(a_1 - b_1, \dots, a_k - b_k, \frac{\mathbf{A}\cdot\mathbf{B}}{\parallel \mathbf{A} \parallel \parallel \mathbf{B} \parallel } \right) \]
where \(\mathbf{A} = (a_1, \dots, a_k) \) and \(\mathbf{B} = (b_1, \dots, b_k) \) are the \(k\)-dimensional embedding vectors for the two courses.  This design is powerful because it provides the subsequent classifier with two distinct types of information simultaneously. The element-wise difference captures granular, dimension-specific (local) disparities between the two semantic representations, while the cosine similarity provides a single, normalized measure of their overall (global) alignment in the vector space. This composite vector creates a much more informative and discriminative feature set than a single similarity score alone.

To generate the data for our models, a \verb|CoursePairGenerator| class was implemented. This utility systematically created the training and test datasets by first assembling positive pairs (courses with the same C-ID label) and negative pairs (courses with different C-ID labels). For each of these pairs, it then computed the composite feature vector \(\Delta_c\), resulting in a highly informative and discriminative feature set ready for classification.

This procedure was applied to every set of embeddings (e.g., BGE-original, BGE-PCA4-reduced, GIST-original, etc.), resulting in a comprehensive suite of training and testing datasets. Each dataset corresponds to a unique combination of an embedding model and a dimensionality reduction technique, ready for evaluation by the downstream classifiers detailed in the next section.

\section{Classification Models}
To determine the most effective method for classifying the generated pairwise feature vectors, a broad suite of machine learning algorithms was evaluated. This process began with a comprehensive initial evaluation of eight different models to understand which algorithmic families were best suited to the data. Based on those preliminary results, a smaller, more focused set of high-performing models was selected for the final, in-depth analysis.

\subsection{Initial Model Evaluation}
The initial evaluation included a diverse set of classifiers, each chosen to test a different hypothesis about the structure of the feature space:
\begin{itemize}
    \item \textbf{Linear Models (Logistic Regression, Ridge, Lasso)}: These models were used to establish a baseline and determine the degree of linear separability of the data. Their simplicity and interpretability make them excellent for understanding the foundational difficulty of the classification task. 
    \item \textbf{Instance-Based Model (k-Nearest Neighbors)}: KNN was included to probe the local structure of the feature space. Its performance indicates whether courses with the same equivalency status form dense, localized clusters.  
    \item \textbf{Kernel-Based Model (Support Vector Machine)}: An SVM with a non-linear kernel was used to test for complex, non-linear decision boundaries that linear models cannot capture.  
    \item \textbf{Ensemble Model (Random Forest)}: Random Forest was chosen for its robustness to overfitting and its ability to capture complex feature interactions by combining the predictions of many decision trees.  
    \item \textbf{Probabilistic Models (LDA and QDA)}: Linear and Quadratic Discriminant Analysis were used to test assumptions about the geometric distribution of the data, specifically whether the classes share a common covariance (LDA) or have unique ones (QDA).   
\end{itemize}

\subsection{Final Classifier Selection}
Based on the preliminary results from the comprehensive evaluation, four models were selected for the final analysis due to their consistently strong performance: K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Random Forest (RF), and XGBoost. XGBoost, a powerful gradient boosting implementation, was added at this stage to include a state-of-the-art boosting algorithm known for its high performance on structured data. These models represent the most promising approaches for this classification task, covering instance-based, maximal-margin, and advanced ensemble methods.

\section{Multi-Stage Evaluation}
A rigorous, multi-stage evaluation framework was designed to systematically narrow down the optimal combination of embedding models, dimensionality reduction techniques, and classifiers.

\subsection{Stage 1: Preliminary Embedding Model Selection}
The process began with a broad analysis of twelve open-source embedding models to identify strong candidates for more intensive study. This initial screening was performed on a small, manually curated dataset. The evaluation metric was a simple accuracy score based on cosine similarity: for a given course, a model was scored as correct if an equivalent course had a higher cosine similarity than a non-equivalent course. This efficient, low-cost evaluation allowed for the rapid elimination of poorly performing models, resulting in the selection of three top candidates representing a range of sizes: BAAI/bge-small-en-v1.5 (BGE), avsolatorio/GIST-Embedding-v0 (GIST), and nvidia/NV-Embed-v2 (NVE).

\subsection{Stage 2: Comprehensive Classifier and Reducer Evaluation}
Using the three selected embedding models and the initial small dataset, a comprehensive GridSearchCV was conducted. This stage systematically tested each embedding model in combination with multiple dimensionality reduction techniques (PCA, t-SNE, and PaCMAP at 4 and 7 dimensions, as well as with 70\%, 80\%, and 90\% explained variance for PCA) and the full suite of eight initial classifiers (Logistic Regression, Ridge, Lasso, KNN, SVM, Random Forest, LDA, and QDA). The goal of this exhaustive search was to identify the most effective downstream classifiers and to understand the impact of dimensionality reduction on performance.

\subsection{Stage 3: Evaluation During Fine-Tuning}
With the larger, more robust PPM dataset, the two most promising non-proprietary models (BAAI/bge-small-en-v1.5 and avsolatorio/GIST-Embedding-v0) were fine-tuned to specialize them for the course equivalency task. To monitor the quality of the learned embeddings during this process, the \verb|BinaryClassificationEvaluator| from the sentence-transformers library was employed. At the end of each training epoch, the evaluator was run on binary-labeled course pairs generated from the training set. The primary metric monitored was Average Precision based on cosine similarity, and the model checkpoint that achieved the highest score on these validation pairs was saved as the best model for that fine-tuning run.

\subsection{Stage 4: Final Downstream Classifier Evaluation}
The final and definitive evaluation was conducted on the held-out test portion of the PPM dataset. This stage used the feature vectors generated from the best-performing embedding models (both the original off-the-shelf versions and the newly fine-tuned versions). These feature sets were then used to train and evaluate the final selection of high-performing classifiers: KNN, Random Forest, SVM, and XGBoost. This ensures an unbiased assessment of the complete pipeline's ability to generalize to new, unseen data. To provide a comprehensive view of model performance, the standard suite of classification metrics was calculated from the confusion matrix: accuracy, precision, recall, and \(F_1\)-Score.

Beyond classification accuracy, the computational efficiency of each model pipeline was assessed by measuring both training and inference times from the hyperparameter cross-validation grid search. While training time provides insight into the resources required to develop a model, inference time is the more critical metric for this research's use case. Inference time directly affects the system's responsiveness and scalability in a production environment, making it a key factor in determining the practical viability of a given solution.

The performance of each of the final classifiers on each of the final feature sets was measured using these metrics. The results of this final evaluation, which form the core findings of this thesis, will be presented and analyzed in the subsequent chapter.