\chapter{Experimental Setup and Results}\label{ch:4}
This chapter presents the empirical results that validate the decoupled, deep metric learning framework proposed in Chapter~\ref{ch:3}. The central objective is to systematically assess the performance of each component of the pipeline—from the choice of embedding model and the impact of fine-tuning to the effects of dimensionality reduction and the selection of a downstream classifier—to identify the optimal configuration for both classification accuracy and computational efficiency.

The analysis is structured to first establish the validity of the core feature engineering approach before proceeding through the primary evaluation sequence. The chapter begins by presenting a critical ablation study to validate the efficacy of the novel composite distance vector (\(\Delta_c\)), the cornerstone of the feature representation. With the feature vector's design validated, the investigation then establishes a baseline by assessing off-the-shelf models, quantifies the performance gains achieved through fine-tuning, and analyzes the trade-offs between accuracy and efficiency. The chapter culminates in a definitive comparative analysis on the held-out test data, synthesizing all prior findings to identify the single best-performing pipeline configuration. We begin by outlining the experimental setup that forms the foundation for all subsequent results.

\section{Experimental Environment \& Datasets}\label{ch:4.1}

\subsection{Computational Environment}\label{ch:4.1.1}
The research presented in this thesis was conducted using a hybrid computational environment, leveraging both cloud-based services for initial language model evaluations and a powerful on-premise high-performance computing (HPC) cluster for the primary, computationally intensive experiments. The initial direct classification tasks were performed using Google's Gemini v1.0, a proprietary cloud-based Large Language Model. All subsequent stages, including model fine-tuning and downstream classifier evaluations, were executed on San Francisco State University's ``POLARIS'' High Performance Compute Cluster. The POLARIS cluster runs on Rocky Linux 8.9 with the Slurm Workload Manager. GPU-intensive deep learning tasks were performed on a node equipped with four NVIDIA A100 GPUs, while extensive hyperparameter grid searches for traditional machine learning classifiers were run on a CPU cluster with AMD EPYC 9534 CPUs. The entire experimental pipeline was implemented in Python, with the core deep learning components built using PyTorch and the Hugging Face ecosystem and the classical machine learning experiments conducted using \verb|scikit-learn| and \verb|XGBoost|.

\subsection{Datasets}\label{ch:4.1.2}
As detailed in Chapter~\ref{ch:3}, this research utilized two distinct datasets at different stages of the evaluation. A smaller \textbf{Initial Dataset}, manually curated from the ASSIST repository, was used for prototyping and the preliminary screening of models and classifiers. The definitive experiments were conducted on the larger \textbf{PPM Corpus}, provided by the Program Pathways Mapper (PPM). This corpus was used for the definitive fine-tuning and final pipeline evaluation, with its held-out test set reserved for an unbiased assessment of the optimized pipelines to ensure methodological rigor. Table~\ref{tbl:datasets} provides a summary of the key characteristics of both datasets.

\begin{table}[!tb]
    \captionsetup{skip=5pt}
    \centering
    \caption{Summary of Datasets Used in Evaluation}
    \label{tbl:datasets}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{p{0.2\textwidth} p{0.4\textwidth} p{0.4\textwidth}}
            \toprule
            \textbf{Characteristic} & \textbf{Initial Dataset}                                      
            & \textbf{PPM Corpus}                                  \\
            \midrule
            \textbf{Source}         & Manually curated via ASSIST                   
            & Program Pathways Mapper (PPM)                        \\
            \addlinespace
            \textbf{Purpose}        & Preliminary screening, prototyping, and initial classifier evaluation & Definitive fine-tuning and final pipeline evaluation \\
            \addlinespace
            \textbf{Ground Truth}   & Established articulation agreements                                   & Course Identification Number (C-ID)                  \\
            \addlinespace
            \textbf{Final Size}     & 400 course pairs (for evaluation set)                                 & 2,157 courses (across 157 classes)                   \\
            \addlinespace
            \textbf{Partitioning}   & Stratified random sample                                              & Stratified 50/50 train/test split                    \\
            \bottomrule
        \end{tabular}
    }
\end{table}

\subsection{Evaluation Metrics}\label{ch:4.1.3}
To facilitate a multi-faceted analysis, a standard suite of metrics was used to assess both classification efficacy and computational efficiency. The core assessment of classification performance was measured using the \textbf{\(F_1\)-Score}, the harmonic mean of Precision and Recall. This metric was chosen because it provides a single, robust value that balances the trade-off between avoiding false equivalencies (precision) and identifying all true ones (recall), making it ideal for this task. For assessing practical viability, both \textbf{Training Time} and \textbf{Inference Time} were systematically captured. Inference time is considered the more critical efficiency metric for this research, as it directly impacts the system's real-world responsiveness and scalability in a production environment.

\section{Baseline Performance: Direct LLM Classification}\label{ch:4.2}
The initial phase of this research sought to establish a performance baseline by evaluating the feasibility of using a Large Language Model for end-to-end course equivalency classification, a methodology detailed in Section~\ref{ch:3.1}. The experiments were conducted on the Initial Dataset, leveraging Google's Gemini Pro v1.0 as the reasoning engine. The results were encouraging, with the model achieving a peak accuracy of 90.5\% when using full, unprocessed raw text as input. This approach consistently yielded superior results compared to using only extracted structured topics. This performance gap is likely attributable to the challenges inherent in the data extraction step itself, which proved to be difficult and highly sensitive to prompt wording---a finding that aligns with previous studies on prompt engineering~\cite{Sclar2023QuantifyingLM,Errica2024WhatDI,Chen2024LLMAA,Cheung2024ARC,Gerych2024WhoKT}.

An analysis of the confusion matrices, presented in Figure~\ref{fig:cm}, reveals a consistent and telling behavioral pattern: the model exhibits a strong conservative bias. Across all scenarios, it was far more likely to misclassify a truly equivalent course pair as not equivalent (a false negative) than it was to incorrectly approve a non-equivalent pair (a false positive), suggesting the LLM operates with a high internal threshold for declaring equivalency. Furthermore, the experiments that introduced ``unsure'' and ``insufficient data'' categories demonstrated the model's ability to effectively isolate ambiguous cases that would require manual review. A comprehensive summary of the performance metrics is provided in Table~\ref{tbl:llm_results_summary}, and a comparative benchmark against other prominent LLMs is shown in Table~\ref{tbl:cls}.
\begin{figure}[tb]
    \captionsetup{skip=5pt}
    \centering
    \begin{subfigure}[b]{0.497\textwidth}
        \centering
        \includegraphics[scale=0.395,trim={3mm 20mm 7mm 0},clip]{LLMeval_binary_raw_text_cm.pdf}
        \caption{Binary Raw Text}
        \label{fig:sub_brt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.497\textwidth}
        \centering
        \includegraphics[scale=0.42,trim={3mm 20mm 7mm 0},clip]{LLMeval_binary_topics_cm.pdf}
        \caption{Binary Topics}
        \label{fig:sub_brt}
    \end{subfigure}

    \vspace{2mm}

    \begin{subfigure}[b]{0.497\textwidth}
        \centering
        \includegraphics[scale=0.42,trim={3mm 20mm 7mm 0},clip]{LLMeval_3way_raw_text_cm.pdf}
        \caption{3-way Raw Text}
        \label{fig:sub_brt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.497\textwidth}
        \centering
        \includegraphics[scale=0.42,trim={3mm 20mm 7mm 0},clip]{LLMeval_3way_topics_cm.pdf}
        \caption{3-way Topics}
        \label{fig:sub_brt}
    \end{subfigure}

    \vspace{2mm}

    \begin{subfigure}[b]{0.497\textwidth}
        \centering
        \includegraphics[scale=0.42,trim={3mm 20mm 7mm 0},clip]{LLMeval_4way_raw_text_cm.pdf}
        \caption{4-way Raw Text}
        \label{fig:sub_brt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.497\textwidth}
        \centering
        \includegraphics[scale=0.42,trim={3mm 20mm 7mm 0},clip]{LLMeval_4way_topics_cm.pdf}
        \caption{4-way Topics}
        \label{fig:sub_brt}
    \end{subfigure}
    \caption{LLM Classification Confusion Matrices}
    % {\footnotesize Clockwise from bottom left: 4-class raw text, 3-class raw text, binary raw text, binary
    %     structured topics, 3-class structured topics, and 4-class structured topics.\\\footnotemark[1]ins: insufficient description}
    \label{fig:cm}
\end{figure}
\begin{table}[!tb]
    \captionsetup{skip=5pt}
    \centering
    \caption{Performance Summary of Direct LLM Classification}
    \label{tbl:llm_results_summary}
    \begin{tabular}{lccccc}
        \toprule
                                  & \textbf{Classification} &                   &                    &                     &                   \\
        \textbf{Input Data}       & \textbf{Task}           & \textbf{Accuracy} & \textbf{F1-Score*} & \textbf{Precision*} & \textbf{Recall*}  \\
        \midrule
        \multirow{3}{*}{Raw Text} & Binary                  & 0.9050            & 0.8973             & 0.9765              & 0.8300            \\
                                  & 3-way                   & 0.8750            & 0.8877             & 0.9818              & 0.8100            \\
                                  & 4-way                   & 0.8100            & 0.8596             & 0.9808              & 0.7650            \\
        \midrule
        \multirow{3}{*}{Topics}   & Binary                  & 0.8100            & 0.7654             & 1.0000              & 0.6200            \\
                                  & 3-way                   & 0.7775            & 0.7795             & 0.9847              & 0.6450            \\
                                  & 4-way                   & 0.7225            & 0.7531             & 0.9839              & 0.6100            \\
        \bottomrule
        \multicolumn{6}{p{0.9\textwidth}}{\scriptsize * F1-Score, Precision, and Recall are reported for the positive class (``Equivalent'').} \\
    \end{tabular}
\end{table}
\begin{table}[tb]
    \captionsetup{skip=5pt}
    \caption{Model Specifications and Performance}
    \label{tbl:cls}
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccccc}
            \toprule
                                            &                      & \textbf{Context}                &                  &                   &
                                            &                      &                                                                                                                     \\
            \textbf{Model Name}             & \textbf{Parameters*} & \textbf{Length}                 & \textbf{Support} & \textbf{Accuracy} &
            \textbf{Precision}              & \textbf{Recall}      & \(\mathbf{F_1}\)\textbf{-score}                                                                                     \\
            \midrule
            Google Gemini Pro 1.0           & Unknown              & 32,768                          & 400              & \textbf{0.9050}   & 0.9765 & \textbf{0.8300} & \textbf{0.8973} \\
            Meta Llama 3.1 8B Instruct      & 8                    & 128,000                         & 208 (52\%)       & 0.6250            & 1.0000 & 0.3500          & 0.5185          \\
            Microsoft Phi 3 Medium Instruct & 14                   & 128,000                         & 400              & 0.7100            & 1.0000 & 0.4200          & 0.5915          \\
            Google Gemma 2 27b              & 27.2                 & 8,000                           & N/A              & N/A               & N/A    & N/A             & N/A             \\
            Meta Llama 3.1 70B Instruct     & 70                   & 128,000                         & 400              & 0.7350            & 1.0000 & 0.4700          & 0.6395          \\
            Qwen 2 72B Instruct             & 72.7                 & 131,072                         & 400              & 0.7650            & 1.0000 & 0.5300          & 0.6928          \\
            Anthracite Magnum v1 72B        & 72.7                 & 32,768                          & 400              & 0.8525            & 1.0000 & 0.7050          & 0.8270          \\
            CalmeRys 78B Orpo v0.1          & 78                   & 32,768                          & 400              & 0.7175            & 1.0000 & 0.4350          & 0.6063          \\
            Mixtral 8x22B Instruct v0.1     & 141                  & 65,536                          & 400              & 0.6450            & 1.0000 & 0.2900          & 0.4496          \\
            Meta Llama 3.1 405B Instruct**  & 405                  & 128,000                         & 400              & 0.7775            & 1.0000 & 0.5550          & 0.7138          \\
            \bottomrule
            \multicolumn{8}{l}{\footnotesize *in Billions}                                                                                                                               \\
            \multicolumn{8}{l}{\footnotesize **INT4 Quantized Model}
        \end{tabular}
    }
\end{table}

This initial study confirmed that while direct LLM classification can achieve high accuracy, it has fundamental limitations, including high computational cost, lack of a quantifiable similarity score, sensitivity to input quality, and an opaque, ``black box'' nature. These challenges validated the decision to pivot and motivated the development of the more robust, decoupled pipeline that is the primary focus of this thesis.

%%%%%%%% need table from edm paper %%%%%%%%%%
\section{Core Component Validation: The Composite Distance Vector}\label{ch:4.3}
A foundational contribution of this research is the composite distance vector, \(\Delta_c\), introduced in Section~\ref{ch:3.2.3.2}. This vector was designed to provide a richer feature set for downstream classifiers by combining granular, dimension-specific differences with a holistic, global cosine similarity metric. To validate this design, a critical ablation study was conducted to quantify the impact of the global cosine similarity term on classifier performance. Classifiers were trained on both the complete vector (\(\Delta_c\)) and an ablated version containing only the local, element-wise differences (\(\Delta_l\)).

The results revealed a stark dichotomy between the behavior of linear and non-linear models. The performance of linear models, such as Logistic Regression, showed a dramatic improvement with the inclusion of the cosine similarity term. For example, the \(F_1\)-score for Logistic Regression surged from 0.686 to 0.901 with the addition of the single global feature. This demonstrates that these simpler models, which are constrained to learning linear decision boundaries, rely on this explicit global feature and are unable to derive the relationship from the local difference vector alone. In contrast, the performance of more complex, non-linear models (e.g., KNN, SVM, and Random Forest) was not significantly affected. Their exceptionally high performance remained stable with or without the cosine similarity term, suggesting these more powerful models are capable of inferring the global relationship directly from the high-dimensional local difference vector.

The results of this ablation study are conclusive regarding the utility of the composite distance vector, \(\Delta_c\). For linear models, its inclusion is critical, providing a substantial boost in performance. While the more powerful non-linear models did not derive a significant benefit, their performance was not negatively impacted. Therefore, to maintain a consistent and robust feature engineering pipeline, the composite distance vector was retained as the standard feature representation for all subsequent experiments. The investigation into alternative composite vectors that could potentially yield improvements for non-linear models is an avenue for future research.

\section{Domain-Specific Adaptation: Embedding Model Fine-Tuning}\label{ch:4.4}
This section details the empirical investigation into enhancing a pre-trained embedding model by fine-tuning it on the domain-specific PPM Corpus. This process, a form of deep metric learning, aims to restructure the embedding space such that geometric distance directly corresponds to semantic similarity. Such an approach is particularly relevant for scenarios with high intra-class variance and low inter-class variance, a common characteristic of specialized domains where fine distinctions are paramount~\cite{mohan2023deepmetriclearningcomputer}.

\subsection{Fine-Tuning Experiment Summary}\label{ch:4.4.1}
To identify the optimal fine-tuning configuration, a systematic evaluation was designed to test the impact of different loss functions and learning rate schedules on the \verb|BAAI/bge-small-en-v1.5| (BGE) base model. A matrix of twelve experimental configurations was executed, crossing the four triplet mining loss functions and three learning rate schedulers whose theoretical underpinnings were detailed in Section~\ref{ch:3.3.1.2}. The training was conducted on the POLARIS HPC cluster using a distributed data-parallel strategy. To ensure efficient and robust training, the \verb|GroupByLabelBatchSampler| was employed for effective triplet mining, and model performance was monitored at the end of each epoch using the \(F_1\)-score on a binary classification task. This was a deliberate choice to evaluate the model on its ultimate downstream application rather than the triplet loss objective itself, providing a more honest assessment of its ability to generalize.

\subsection{Empirical Results}\label{ch:4.4.2}
The systematic evaluation of the twelve fine-tuning configurations produced a rich set of performance data. Figure~\ref{fig:binaryf1} visualizes the validation \(F_1\)-score over the training process, providing a qualitative comparison of the learning dynamics for each configuration. For a more precise and formal comparison, the exact peak validation \(F_1\)-scores achieved by each configuration are summarized in Table~\ref{tab:finetune_f1_scores}.
\begin{figure}[tb]
    \captionsetup{skip=5pt}
    \centering
    \includegraphics[scale=0.15,trim={0 0 0 0},clip]{fine-tune_validation_f1_vs_step.png}
    \caption{Validation \(F_1\)-Score}
    % {\footnotesize Clockwise from bottom left: 4-class raw text, 3-class raw text, binary raw text, binary
    %     structured topics, 3-class structured topics, and 4-class structured topics.\\\footnotemark[1]ins: insufficient description}
    \label{fig:binaryf1}
\end{figure}
\begin{table}[tb]
    \captionsetup{skip=5pt}
    \centering
    \caption{Peak Validation F1-Score of Fine-Tuning Configurations on BGE Model}
    \label{tab:finetune_f1_scores}
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{l c c c}
            \toprule
            \textbf{Loss Function}                          & \textbf{Linear Scheduler} & \textbf{Cosine Restarts v1} & \textbf{Cosine Restarts v2} \\
            \midrule
            \texttt{BatchAllTripletLoss} (atl)              & 0.8076                    & 0.8078                      & 0.8082                      \\
            \texttt{BatchHardSoftMarginTripletLoss} (hsmtl) & 0.8094                    & 0.8099                      & 0.8098                      \\
            \texttt{BatchHardTripletLoss} (htl)             & 0.8093                    & 0.8097                      & 0.8096                      \\
            \texttt{BatchSemiHardTripletLoss} (shtl)        & 0.8088                    & \textbf{0.8104}             & 0.8088                      \\
            \bottomrule
        \end{tabular}
    }
\end{table}

The quantitative data reveals a nuanced outcome. While aggressive hard-negative mining strategies such as \verb|BatchHardTripletLoss| and \verb|BatchHardSoftMarginTripletLoss| consistently provided a high performance floor, the single best-performing configuration was the combination of \verb|BatchSemiHardTripletLoss| with the \verb|cr_v1| cosine annealing schedule, which achieved the highest peak validation \(F_1\)-score of 0.8104. This result suggests that a more moderate mining strategy, when paired with a patient and robust optimization schedule, can find a slightly better optimum for this specific task.

\subsection{Statistical Validation and Selection of the Optimal Model}\label{ch:4.4.3}
The superior performance of the winning configuration arises from a synergistic framework: a stable learning objective from semi-hard mining, a robust exploration strategy from the \verb|CosineAnnealingWarmRestarts| scheduler that allows the optimizer to escape local minima, and effective regularization from the AdamW optimizer~\cite{loshchilov2019decoupledweightdecayregularization,hermans2017defensetripletlossperson,Schroff_2015_CVPR,loshchilovhutter}.

To definitively validate the benefit of this fine-tuning process, the performance of the resulting model (bge-ft) was compared against the suite of off-the-shelf models using the held-out test portion of the PPM Corpus. The fine-tuned bge-ft model not only achieved the highest mean \(F_1\) test score (\(\mu = 0.9786\)) but also exhibited the lowest standard deviation (\(\sigma = 0.0045\)), indicating it is both more accurate and more consistent on unseen data. A one-way Analysis of Variance (ANOVA) confirmed that a statistically significant difference exists between the model groups (\(F=6.2856, p<0.001\)). A subsequent Games-Howell post-hoc test, justified by a significant Levene's test for unequal variances, revealed that the bge-ft model demonstrated a statistically significant improvement over its base model, bge, with a mean \(F_1\)-score increase of 0.0109 (\(p<.001, 95\% CI [0.0054, 0.0164]\)). Notably, bge-ft significantly outperformed all other evaluated models, including those that were orders of magnitude larger (\(p=0.0009\) vs. gist; \(p=0.0003\) vs. nve; \(p=.0115\) vs. sfr).

This rigorous evaluation leads to a clear and unambiguous conclusion. The single best model for this task is the \verb|BAAI/bge-small-en-v1.5| model fine-tuned using \verb|BatchSemiHardTripletLoss| and a \verb|CosineAnnealingWarmRestarts| learning rate schedule. This model, hereafter designated as \textbf{bge-ft}, was proven to be statistically superior to all off-the-shelf models evaluated in this study and is carried forward as the primary fine-tuned embedding model for the comprehensive classifier evaluation detailed in the next section.

\section{Final Stage Evaluation: Downstream Classifier Performance}\label{ch:4.5}
This section presents the definitive evaluation of the downstream classifiers, representing the final component of the proposed framework. The objective is to systematically identify the optimal classification model by balancing predictive accuracy against computational efficiency. The evaluation proceeds from a broad preliminary review to a focused analysis of four finalist classifiers on the PPM Corpus, culminating in a statistical comparison on held-out test data.

\subsection{Preliminary Classifier Performance Review and Finalist Selection}\label{ch:4.5.1}
The initial assessment, conducted on the Initial Dataset, tested eight different classifiers against feature sets derived from the off-the-shelf embedding models. This wide-ranging review established that non-linear models (KNN, SVM, RF) performed exceptionally well, suggesting they could effectively capture the complexity of the feature space. In contrast, linear models showed a profound dependency on the engineered cosine similarity feature, highlighting their limitations.

The preliminary review also systematically tested multiple dimensionality reduction techniques, including PCA, t-SNE, and PaCMAP. For the top-performing non-linear models (KNN, SVM, RF), applying dimensionality reduction generally had a neutral or slightly negative impact on performance. This suggests that while the techniques reduce complexity, they may also discard some useful, discriminative information contained within the original high-dimensional embeddings, as evidenced by instances where scores dropped after reduction was applied.

Based on these preliminary results, the most robust and successful classifiers—K-Nearest Neighbors (KNN), Support Vector Machine (SVM), and Random Forest (RF)—were selected as finalists. To ensure the final comparison was comprehensive, XGBoost was added to this slate to include a state-of-the-art gradient boosting algorithm.

\subsection{Efficacy vs. Efficiency Analysis}\label{ch:4.5.2}
Transitioning to the PPM Corpus and using features generated by the superior \textbf{bge-ft} model, the analysis next investigated the crucial trade-off between model efficacy and computational efficiency for the four finalist classifiers. Figure~\ref{fig:f1time} visualizes the median \(F_1\)-scores from the cross-validation grid search against both training and inference times. While all four models demonstrate the ability to achieve high accuracy, Random Forest and XGBoost emerge as the clear winners on the critical metric of inference time. Their inference times are tightly clustered under 0.1 seconds—an order of magnitude faster and more predictable than the wider, more variable times of KNN and SVM. This reliability makes RF and XGBoost inherently more robust choices for a scalable, low-latency production system.
\begin{figure}[tb]
    \captionsetup{skip=5pt}
    \centering
    \includegraphics[scale=0.85,trim={0 5mm 0 18mm},clip]{cv_f1 vs time.pdf}
    \caption{Classifier \(F_1\)-Score vs. Training and Inference Time}
    % {\footnotesize Clockwise from bottom left: 4-class raw text, 3-class raw text, binary raw text, binary
    %     structured topics, 3-class structured topics, and 4-class structured topics.\\\footnotemark[1]ins: insufficient description}
    \label{fig:f1time}
\end{figure}

\subsection{Statistical Analysis of Classifier Performance on Test Data}\label{ch:4.5.3}
The definitive comparison was conducted on the held-out test data from the PPM Corpus. As shown in the score distributions (Figure~\ref{fig:clscoredist}) and descriptive statistics (Table~\ref{tbl:clsdescstat}), all four classifiers demonstrate exceptionally high and stable performance. Notably, the Support Vector Machine (SVM) model achieved both the highest mean \(F_1\)-score (\(\mu = 0.975973\)) and the lowest standard deviation (\(\sigma = 0.009120\)), indicating it is the most accurate and consistent performer.
\begin{table}[tb]
    \captionsetup{skip=5pt}
    \caption{Classifier Descriptive Statistics}
    \label{tbl:clsdescstat}
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lcccccccc}
            \toprule                                                                                        \\
            Classifier & Count & Mean     & Std      & Min      & 25\%     & 50\%     & 75\%     & Max      \\
            \midrule
            KNN        & 80.0  & 0.971147 & 0.015208 & 0.900304 & 0.971809 & 0.977110 & 0.979110 & 0.982437 \\
            RF         & 80.0  & 0.970397 & 0.014636 & 0.934443 & 0.957361 & 0.978490 & 0.982108 & 0.984572 \\
            SVM        & 80.0  & 0.975973 & 0.009120 & 0.939885 & 0.970523 & 0.979408 & 0.982786 & 0.987702 \\
            XGB        & 80.0  & 0.971026 & 0.012316 & 0.942860 & 0.959382 & 0.977547 & 0.981376 & 0.984898 \\
            \bottomrule
        \end{tabular}
    }
\end{table}
\begin{figure}[tb]
    \captionsetup{skip=5pt}
    \centering
    \includegraphics[scale=0.75,trim={0 5mm 0 18mm},clip]{classifier_f1score_boxplot_test.pdf}
    \caption{\(F_1\)-Score Distribution of Finalist Classifiers}
    % {\footnotesize Clockwise from bottom left: 4-class raw text, 3-class raw text, binary raw text, binary
    %     structured topics, 3-class structured topics, and 4-class structured topics.\\\footnotemark[1]ins: insufficient description}
    \label{fig:clscoredist}
\end{figure}

To determine if these observed differences were statistically significant, a one-way Analysis of Variance (ANOVA) was performed, which confirmed a significant difference among the classifiers (\(F(3, 316) = 3.1293, p = 0.02596\)). A subsequent Games-Howell post-hoc test, justified due to unequal variances, revealed that the SVM classifier performed statistically significantly better than both Random Forest (mean difference \(= 0.0056, p=0.0229\)) and XGBoost (mean difference \(= 0.0049, p=0.0229\)). While all four are top-tier performers, the formal statistical analysis identifies SVM as the definitive winner in terms of raw accuracy.

\subsection{Final Classifier Selection}\label{ch:4.5.4}
This multi-stage evaluation reveals a classic trade-off between peak performance and operational efficiency. The statistical superiority of SVM is clear; however, it corresponds to a mean \(F_1\)-score improvement of only ~0.5\% over the far more efficient tree-based models. The final choice is therefore context-dependent. Based on the comprehensive evidence, the following recommendations are made:
\begin{itemize}
    \item \textbf{For Maximum Accuracy}: The Support Vector Machine (SVM) is the recommended classifier for any scenario where achieving the absolute highest accuracy and consistency is the paramount objective.
    \item \textbf{For Optimal Efficiency}: Random Forest and XGBoost are compelling and practical alternatives for applications where inference speed, low latency, and predictable performance are critical for scalability. They provide nearly equivalent accuracy with demonstrably superior and more reliable computational efficiency.
\end{itemize}

\section{Qualitative Diagnosis: Misclassification Analysis}\label{ch:4.6}
While quantitative metrics are essential for benchmarking and model selection, they obscure the underlying nature of a model's failures. Relying on aggregate scores alone can be misleading, as they may overestimate a model's robustness while hiding significant, systematic failure modes~\cite{gauthier2022}. This section, therefore, transitions from quantitative assessment to a qualitative diagnosis of the misclassifications produced by the embedding models to understand their root causes. The analysis examines shared misclassifications across models and conducts a granular, case-based review of false positives and false negatives.

\subsection{Shared Misclassifications Across Models}\label{ch:4.6.1}
To diagnose the source of model errors, it is first necessary to determine whether they are idiosyncratic to individual models or systematic across them. A significant overlap in misclassified pairs across multiple, diverse models would suggest the presence of errors that stem not from the models but from the data itself, such as inherent ambiguity or annotation artifacts.

The analysis of misclassifications on the held-out test data reveals a high degree of overlap, providing strong evidence for the prevalence of such systematic errors. As illustrated in Figure~\ref{fig:venn_misclassified}, a total of 211 distinct course pairs were misclassified by every single embedding model evaluated, from the large, general-purpose models to the small, domain-specific \textbf{bge-ft}. The intersection bar charts in Figure~\ref{fig:intersection_barchart} reinforce this finding, showing high counts of shared errors across all model combinations. The interpretation is clear: a significant portion of the model failures are not random but are systematic products of the course catalog corpus. These ``hard'' examples consistently challenge a range of semantic embedding models and may define the performance ceiling of any approach relying solely on course descriptions.
\begin{figure}[tb]
\centering
\includegraphics[width=0.8\textwidth]{venn_with_totals_evaldata.png}
\caption{Venn diagram illustrating the overlap of misclassified pairs on the test dataset between all five embedding models. The central intersection shows 211 pairs were misclassified by all models, indicating systematic data challenges.}
\label{fig:venn_misclassified}
\end{figure}
\begin{figure}[tb]
\centering
\includegraphics[width=\textwidth,trim={0 5mm 0 18mm},clip]{intersection_barchart_evaldata.png}
\caption{Number of common misclassified pairs between all model combinations on the test dataset. The high counts of shared errors across different models point to systematic challenges inherent in the data.}
\label{fig:intersection_barchart}
\end{figure}

\subsection{Qualitative Analysis of False Positives}\label{ch:4.6.2}
A False Positive (FP) occurs when two non-equivalent courses are incorrectly classified as equivalent. This type of error carries significant real-world consequences, as it could lead a student to waste time and tuition on a non-transferable course. The analysis identified two primary causes:
\begin{itemize}
    \item \textbf{Topical Overlap without True Equivalence:} This error arises when courses cover the same broad subject but differ in critical dimensions such as academic level or their position in a curricular sequence. The model correctly identifies high semantic similarity but fails to capture the nuanced distinctions that make the courses non-equivalent. An example is the pair of PHYS-4D and PHYS-4A from Foothill College, which are sequential calculus-based physics courses covering modern and classical mechanics, respectively. Their shared title and subject matter create high semantic overlap, but they are not equivalent for transfer purposes.
    \item \textbf{Ambiguous or Vague Course Descriptions:} This occurs when descriptions are too brief or use generic language, lacking the specific detail needed for differentiation. This is a known challenge in the field of short-text semantic similarity, where a lack of rich context inherently increases ambiguity~\cite{app13063911}. For example, the description for COMM-1 at Saddleback College uses abstract language like ``processes of communication'' and ``development of ideas,'' which could be semantically close to a wide range of introductory courses in public speaking, rhetoric, or even philosophy.
\end{itemize}

\subsection{Qualitative Analysis of False Negatives}\label{ch:4.6.3}
A False Negative (FN) occurs when the system fails to identify a true equivalence, representing a missed opportunity that could cause a student to unnecessarily retake a course. These errors are predominantly a consequence of inconsistencies and information gaps in the source data itself:
\begin{itemize}
    \item \textbf{Semantic Divergence in Descriptions:} This error occurs when two officially equivalent courses are described using vastly different terminology, phrasing, or pedagogical focus. For example, a pair of courses with the same C-ID (CDEV-100) are described differently: one (Foothill College's CHLD-2) uses the language of traditional developmental psychology (``milestones,'' ``psychosocial''), while the other (Cerritos College's CD-110) uses the language of social justice pedagogy (``diversity and inclusion,'' ``anti-bias curriculum''). The model correctly assesses the texts as semantically dissimilar; the failure lies in the inconsistency of the source data, where the ground truth asserts an equivalence not present in the text.
    \item \textbf{Incomplete or Minimalist Descriptions:} This error arises when one or both course descriptions in an equivalent pair are too sparse to provide sufficient textual signal for the model to establish a confident match. An example is a pair of English literature courses (both C-ID ENGL-120), where one description from Bakersfield College is rich with keywords like ``critical analysis'' and ``prose fiction,'' while the corresponding description from Santa Barbara City College is a brief, high-level survey summary, lacking the conceptual depth for the model to map the two courses closely.
    \item \textbf{Data Quality and Labeling Errors:} In some cases, a false negative can be traced back to a fundamental error in the ground-truth data. A striking example is a pair of courses labeled with C-ID SOCI-125, where the description for one course is for an LGBTQ+ studies class, while the description for the other is for an introduction to statistics. This is almost certainly a data entry error in the source corpus. The model correctly identifies the courses as non-equivalent but is penalized with a false negative because the ground-truth label is erroneous.
\end{itemize}

\section{Summary}\label{ch:4.7}
This chapter presented a comprehensive, multi-stage empirical evaluation that validated the proposed decoupled, deep metric learning framework. The investigation began by establishing a performance baseline using a direct Large Language Model approach, which, while capable, highlighted critical limitations that motivated the development of the primary pipeline. The subsequent validation of the framework's core components proved highly successful. A critical ablation study confirmed that the novel composite distance vector, \(\Delta_c\), is a demonstrably superior feature representation.

The most significant performance gain was achieved through domain-specific fine-tuning. The resulting \textbf{bge-ft} model, adapted to the PPM Corpus using a \verb|BatchSemiHardTripletLoss| objective, was shown to be statistically superior to all off-the-shelf models, providing strong empirical evidence for the efficacy of applying deep metric learning to engineer a bespoke embedding space for the course catalog domain. In the final evaluation stage, while all four finalist classifiers achieved exceptionally high \(F_1\)-scores, a nuanced trade-off between peak accuracy and computational efficiency emerged. Statistical analysis identified the Support Vector Machine (SVM) as the most accurate and consistent classifier, whereas Random Forest and XGBoost proved to be significantly more efficient, making them compelling alternatives for real-world deployment.

Finally, a qualitative misclassification analysis provided a essential perspective on the system's performance. The analysis revealed that despite the high accuracy of the optimized pipeline, the remaining errors are not random but are largely systematic, stemming from challenges inherent to the data itself, such as semantic divergence in the descriptions of equivalent courses, ambiguity from vague descriptions, and fundamental data quality errors. The comprehensive evaluation, therefore, concludes that while the proposed framework is highly effective, the primary bottleneck for further improvement likely does not lie in the model architecture or classification algorithm, but in the quality and consistency of the source data.