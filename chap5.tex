\chapter{Discussion, Future Work, and Conclusion}
This chapter transitions from the empirical validation of the proposed framework to a broader discussion of its implications. Having systematically evaluated each component and demonstrated the model's high performance in Chapter~\ref{ch:4}, this chapter now seeks to interpret the significance of these results in the context of the course articulation problem. The objective is to discuss the key findings, acknowledge the inherent limitations of the study, and outline promising avenues for future research that build upon this work. The chapter will culminate in a final conclusion, summarizing the primary contributions of the thesis and reiterating its significance for fostering a more equitable and efficient higher education ecosystem.

\section{Discussion of Results}\label{ch:5.1}
The empirical results presented in Chapter~\ref{ch:4} offer a robust validation of the core hypothesis of this thesis: that a decoupled, deep metric learning framework can overcome the limitations of prior automated approaches to course articulation. This section discusses the significance of these findings, focusing on the vindication of the proposed framework, the critical impact of domain-specific fine-tuning, and the resulting shift in focus from model-centric to data-centric challenges.

\subsection{A Vindicated Framework}\label{ch:5.1.1}
The proposed pipeline successfully addresses the distinct challenges that have hindered previous attempts at automation. By relying exclusively on publicly available course catalog data, the framework is a privacy-conscious alternative to enrollment-based methods like \emph{course2vec}, which are constrained by their need for sensitive, proprietary student records and are not generalizable to institutions with no prior transfer history~\cite{PardosCourse2Vec2019, slade10.1177/0002764213479366}. Furthermore, by decoupling semantic representation from classification, the framework is substantially more scalable, efficient, and interpretable than using large language models for direct, end-to-end classification. It avoids the high computational costs, ``black box'' opacity, and prompt sensitivity associated with direct LLM approaches while still providing a quantifiable similarity score for each pair. Finally, its use of deep contextual embeddings represents a fundamental advance over older statistical methods like TF-IDF, which lack any true semantic understanding and cannot grasp synonymous or related concepts~\cite{AIZAWA200345}.

\subsection{The Impact of Domain-Specific Fine-Tuning}\label{ch:5.1.2}
Perhaps the most prominent finding from the experimental evaluation is the statistical superiority of the fine-tuned \textbf{bge-ft} model over all off-the-shelf competitors, including those that are orders of magnitude larger. This result provides powerful evidence that for specialized domains, targeted adaptation is more effective than sheer scale. General-purpose models, despite being trained on vast swaths of the internet, lack the specialized ``vocabulary'' to appreciate the fine-grained distinctions in course catalog text. For example, they may not understand the subtle but critical differences between a ``survey'' course, an ``introductory'' course, and a ``foundations'' course. The process of fine-tuning with a triplet loss objective effectively retrained the model's attention mechanism, teaching it the specific semantics and nuances of the academic domain. This allowed it to generate a far more discriminative embedding space, ultimately leading to higher accuracy in the downstream classification task.

\subsection{The Bottleneck Has Shifted from Model-Centric to Data-Centric}\label{ch:5.1.3}
With the optimized pipeline achieving \(F_1\)-scores exceeding 0.98, the framework has pushed the limits of what can be achieved with the available data. The qualitative misclassification analysis in Section~\ref{ch:4.6} revealed that the vast majority of the remaining errors are not due to failures in the model's semantic understanding. Instead, they are artifacts of the source data itself. The model fails when officially equivalent courses are described with vastly different pedagogical language (semantic divergence) or when descriptions are too vague or minimalist to contain a clear signal. In these cases, the model is performing correctly—it accurately reports that the texts are not semantically similar. The error lies in the ground-truth expectation that an equivalence should be found where none is textually supported.  This leads to a critical insight: the primary bottleneck for achieving near-perfect automation has shifted from being model-centric to data-centric. Simply using a larger or more complex model is unlikely to resolve these data-inherent issues. 

This finding serves as a compelling, domain-specific case study for the Data-Centric AI (DCAI) paradigm~\cite{zha2023datacentricartificialintelligencesurvey,wang2025datacentricaicomprehensivesurvey}. Championed by distinguished figures like Andrew Ng, DCAI posits that for many practical applications, once a sufficiently robust model architecture is established, the most consequential and efficient path to performance improvement comes from systematically engineering the data itself~\cite{ngdatacentric_2021,Strickland_2023}. This involves improving label consistency, cleaning noisy examples, and ensuring the dataset accurately reflects the problem domain~\cite{zha2023datacentricartificialintelligencesurvey,ying2025surveydatacentricaitabular}. The results of this thesis validate the DCAI philosophy: state-of-the-art performance was achieved with a well-chosen but not exotic model, and the remaining errors are artifacts of the source data. Further model-tweaking is unlikely to resolve these data-inherent issues; the most promising path forward is to improve the data itself. This suggests that the most promising path to further improvement lies not in novel architectures, but in methodologies that directly address the quality and consistency of the input data~\cite{gauthier2022}, a point that will be explored further in the following sections.

\section{Limitations of the Current Study}\label{ch:5.2}
While the proposed framework represents a pronounced advance, it is essential to acknowledge the limitations that define the boundaries of the current study. These limitations, primarily rooted in the nature of the data and the scope of the task, provide critical context for the results and inform the directions for future work.

\subsection{Data Quality as a Performance Ceiling}\label{ch:5.2.1}
The primary limitation, as identified in the discussion, is that the framework's performance is fundamentally capped by the quality and content of the public course descriptions. The system can only analyze the text that is provided; it cannot infer information that is absent. As the misclassification analysis demonstrated, when course descriptions are vague, minimalist, or use semantically divergent language to describe functionally equivalent courses, the model's ability to determine a correct match is severely hindered. This reliance on the source text means the system is vulnerable to inconsistencies and information gaps in how institutions write and publish their catalogs.

\subsection{Generalizability of the Fine-Tuned Model}\label{ch:5.2.2}
The \textbf{bge-ft} model was fine-tuned and evaluated on a corpus drawn exclusively from California's public colleges and universities. While the framework itself is general, the specific fine-tuned model has been specialized for the linguistic patterns, terminology, and pedagogical styles common to this system. Its performance may not be as high "out-of-the-box" on data from private institutions or different state systems or non-domestic systems, which may have distinct catalog writing conventions. Achieving similar performance in a new institutional context would likely require re-tuning the model on a sample of local data.

\subsection{Scope of "Equivalency"}\label{ch:5.2.3}
This research operationalizes course equivalency as a function of the semantic similarity of their catalog descriptions. This is a powerful proxy, but it does not encompass the full spectrum of factors that human articulation officers may consider. Decisions made by faculty and administrators can be influenced by factors beyond the written content, such as the rigor of the assessment methods, specific lab equipment, faculty credentials, or overarching institutional agreements. The current model is not designed to capture this external, non-textual context.

\subsection{Handling of Complex Articulation Rules}\label{ch:5.2.4}
The framework simplifies the articulation task into a binary classification of course pairs (equivalent or not-equivalent). This approach does not natively handle the more complex articulation scenarios that exist in practice, such as one-to-many (e.g., one university course is equivalent to two community college courses), many-to-many, or non-symmetrical agreements. While the similarity scores produced by the system could inform the discovery of such relationships, the current classification pipeline is not designed to identify them directly, a challenge that persists for many automated systems~\cite{pardos10.1145/3330430.3333622}.

\section{Future Work}\label{ch:5.3}
The findings and limitations of this study give rise to several promising avenues for future research. These directions aim to address the remaining challenges by improving the quality of the input data, expanding the framework's capabilities, and exploring more advanced modeling techniques.

First of all, the framework itself can be expanded to be more useful and tunable for institutional needs. In fact, development is currently underway to evolve the framework beyond binary classification and build a full-scale course recommendation engine, which is being actively researched and developed by our team and will provide a conversational interface for a more intuitive user experience. Such a system would use the vector similarity scores to provide students and advisors with a ranked list of the most likely equivalent courses at a target institution. Furthermore, future work could investigate the classifier's decision threshold as a mechanism for institutional control, allowing administrators to tune the system's behavior from a more lenient stance to one that errs on the side of caution. To address the limitation of complex articulations, another avenue of research involves modeling curricula as graphs and applying graph neural networks to identify one-to-many and many-to-many relationships.

In addition, there remain opportunities to refine the core machine learning components of the pipeline. Future work could systematically investigate alternative composite distance measures and feature combinations to determine if a more optimal representation exists for the downstream classifiers. This could be complemented by exploring multi-modal learning, extending the model to analyze not just the catalog description but also the full text of course syllabi or textbook lists. Likewise, more room still remains to explore task-optimized loss functions for fine-tuning.  Because the embedding vectors are not being used directly, but as upstream feature engines, creating loss functions that behave more similarly to the downstream classifier may have a significant impact on training efficacy.  As models evolve, more sophisticated training methods, such as instruction-tuning on a small, expert-curated dataset, could also be employed to teach the model the explicit task of explaining course equivalency, potentially yielding more interpretable results.

Given that data quality was identified as the primary performance bottleneck, a promising future direction involves implementing a Human-in-the-Loop (HITL) Machine Learning system~\cite{WU2022364}. Rather than simply having an expert review ambiguous cases, a formal HITL system would create an iterative process that promotes continuous improvement~\cite{10.5555/3061053.3061219,wang2019}. In this paradigm, the model would flag low-confidence predictions and route them to a human articulation officer. The expert's decision would not only resolve the immediate case but would also generate a new, high-quality, expert-verified labeled data point. This new data can then be fed back into the training set to periodically re-fine-tune the model, making it progressively smarter and more aligned with expert judgment over time~\cite{Settles2009ActiveLL,wang2022humanintheloopmachinelearningmacromicro}. This approach directly addresses the data-centric bottleneck while building institutional trust by positioning the AI as a tool that augments, rather than replaces, human expertise.

To address the limitation of complex articulation rules, a promising direction for future research is to model the problem using Graph Neural Networks (GNNs)~\cite{kipf2017semisupervisedclassificationgraphconvolutional, hamilton2018inductiverepresentationlearninglarge}. An academic curriculum is not a mere collection of courses; it is an intricate graph where courses are nodes and relationships like prerequisites are the edges that connect them~\cite{zhang2023curriculum, wang2025generativecontrastiveheterogeneousgraphneural}. Standard NLP models, including the one in this thesis, treat course descriptions as isolated documents, ignoring this rich structural information. GNNs, by contrast, are explicitly designed to learn from both the features of the nodes (e.g., the text embedding of a course) and the structure of the graph itself~\cite{veličković2018graphattentionnetworks, s23084168}. A GNN-based approach would learn an embedding for each course that is a function of both its own textual content and the embeddings of its neighbors in the curriculum graph. This structurally-aware embedding could capture the understanding that "Calculus I" followed by "Calculus II" at a community college forms a sequence that, together, might be equivalent to a single "Calculus for Engineers" course at a university. This would allow the system to directly identify these complex, multi-course articulation pathways, a compelling step beyond the current framework's capabilities~\cite{Yan_2024, zhang2023curriculum}.

\section{Conclusion}\label{ch:5.4}
The manual process of determining course equivalency remains a substantive impediment to student mobility in higher education, creating administrative burdens and systemic inequities that lead to student credit loss and delayed graduation~\cite{gao2017, collegeopportunity2017}. This thesis confronted this challenge by designing, developing, and validating a novel computational framework that successfully automates course articulation using only publicly available data. The work's primary contribution is a privacy-preserving, scalable, and computationally efficient pipeline that overcomes the limitations of previous automated approaches. This was achieved through two key technical innovations: the application of deep metric learning to fine-tune a bespoke embedding model for the specific semantics of the academic domain, and the design of a novel composite distance vector that provides a richer feature set for downstream classification.

The result of this research is a highly accurate framework, capable of achieving state-of-the-art performance on a real-world dataset. More importantly, it represents a practical tool that institutions can use to reduce administrative workload, provide faster and more consistent guidance to students, and ultimately foster a more transparent and equitable educational ecosystem. By mitigating the barriers faced by transfer students, particularly those from underrepresented backgrounds~\cite{ace2025}, this work contributes a meaningful step toward fulfilling the promise of accessible and efficient pathways through higher education.