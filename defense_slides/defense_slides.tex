\documentclass[aspectratio=169,10pt]{beamer}
\usetheme{Madrid}
\usecolortheme{default}

% \setbeameroption{hide notes} % Only slides
%\setbeameroption{show only notes} % Only notes
\setbeameroption{show notes on second screen=right} % Both

%------------------------------------------------------------
% Required packages
%------------------------------------------------------------
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{physics}    
\usepackage{comment}    
\usepackage{color}      
\usepackage{listings}
\usepackage{csquotes}
\PassOptionsToPackage{hyphens}{url}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{tikz}

\graphicspath{ {../figures/} }

\usetikzlibrary{arrows.meta, positioning, shapes.geometric}

% Current section
% \AtBeginSection[ ]
% {
% \begin{frame}{Outline}
%     \tableofcontents[currentsection]
% \end{frame}
% }

%------------------------------------------------------------
% Bibliography setup (using biblatex)
% Assumes 'references.bib' is in the parent directory
%------------------------------------------------------------
\usepackage[style=numeric,backend=biber]{biblatex}
\addbibresource{../references.bib}
% \renewcommand*{\bibfont}{\small}

%------------------------------------------------------------
% Title page information
%------------------------------------------------------------
\title[Automating Course Articulation]{Automating Course Articulation:\\A Deep Metric Learning Framework Using Public Data}
\author{Mark S. Kim}
\institute[SFSU]{San Francisco State University \\ Department of Data Science and Artificial Intelligence}
\date{\today}
% First draft started 7-8-2025

%------------------------------------------------------------
% Presentation slides
%------------------------------------------------------------
\begin{document}

\section{Introduction}
% The Title Slide (Slide 1.1)
% \begin{frame}
%     \titlepage
% \end{frame}

% % The Transfer Maze (Slide 1.2)
% \begin{frame}
%     \frametitle{The Problem: The Transfer Maze}
    
%     \begin{columns}[T] % Splits the frame into columns
        
%         \begin{column}{0.6\textwidth} % Left column for text
%             \begin{itemize}
%                 \item The process for determining course equivalency, or \textbf{articulation}, is a formidable, largely manual process that creates significant barriers for students~\cite{pardos2019}.
                
%                 \item In California's public system alone, articulation officers at \textbf{149 individual campuses} manually negotiate and update agreements~\cite{assistfaq, ppic, calstate, cccco}.
                
%                 \item This task of ``bleak combinatorics'' is inefficient, slow, and inherently intractable, struggling to keep pace with the needs of a vast and mobile student body~\cite{pardos2019}.
                
%                 \item This is not a niche issue; transferring between institutions has become a normative part of the modern student's academic journey~\cite{publicagenda2025}.
%             \end{itemize}
%         \end{column}
        
%         \begin{column}{0.4\textwidth} % Right column for a graphic
%             % --- SUGGESTED GRAPHIC ---
%             % A diagram here would be highly effective.
%             % Consider a graphic that visually represents the
%             % "combinatorial explosion" of articulation agreements.
%             %
%             % For example:
%             % - On the left: A large number of circles representing Community Colleges.
%             % - On the right: A smaller number of squares representing CSU/UC campuses.
%             % - A tangled web of lines connecting them, illustrating the
%             %   complexity of one-to-one manual agreements.
%             % - Title it "The Combinatorics of Articulation".
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for the graphic
            
%         \end{column}
        
%     \end{columns}
% \end{frame}

% % The High Cost to Students & Institutions (Slide 1.3)
% \begin{frame}
%     \frametitle{The High Cost to Students \& Institutions}
    
%     \begin{alertblock}{Consequences of an Inefficient System}
%         The administrative friction of the transfer process creates a cascade of negative consequences that fall almost entirely on students.
%     \end{alertblock}
    
%     \begin{columns}[T]
%         \begin{column}{0.6\textwidth}
%             \begin{itemize}
%                 \item \textbf{Significant Credit Loss:} On average, transfer students lose an estimated \textbf{43\%} of their academic credits~\cite{gao2017, publicagenda2025}.
                
%                 \item \textbf{Increased Time-to-Degree:} Lost credits directly delay graduation and postpone entry into the workforce~\cite{gao2017}.
                
%                 \item \textbf{Greater Financial Burden:} Repeating courses increases tuition costs and can exhaust a student's financial aid eligibility~\cite{gao2017, collegeopportunity2017}.
                
%                 \item \textbf{Reduced Student Persistence:} The frustration of the process contributes to lower graduation rates for transfer students compared to their non-transfer peers~\cite{porter1999}.
%             \end{itemize}
%         \end{column}
        
%         \begin{column}{0.4\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % An infographic would be very powerful here.
%             % You could create a simple visual with three key stats:
%             %
%             % 1. A large "43%" with an icon of a diploma tearing.
%             %    Label: "CREDITS LOST"
%             %
%             % 2. A large clock or calendar icon with an arrow showing time increasing.
%             %    Label: "DELAYED GRADUATION"
%             %
%             % 3. A large money bag icon with a minus sign.
%             %    Label: "INCREASED COST"
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for the graphic
            
%         \end{column}
%     \end{columns}
    
%     \note[item]{This high rate of credit loss often forces students to repeat courses for which they have already received a passing grade.}
%     \note[item]{This also increases their overall time in the educational system.}
%     \note[item]{This means a process often undertaken to save money can paradoxically result in a greater overall financial commitment.}
%     \note[item]{The frustration also has a measurable impact on student morale.}
    
% \end{frame}

% % A Critical Equity Issue (Slide 1.4)
% \begin{frame}
%     \frametitle{A Critical Equity Issue}
    
%     \begin{alertblock}{This is not just an administrative problem; it's an equity problem.}
%         The barriers imposed by an inefficient articulation system fall most heavily on the very students institutions are striving to support~\cite{ace2025}.
%     \end{alertblock}
    
%     \begin{columns}[T]
%         \begin{column}{0.6\textwidth}
%             \begin{itemize}
%                 \item Low-income and underrepresented students disproportionately rely on transfer pathways from community colleges~\cite{ace2025}.
                
%                 \item Recent transfer enrollment growth has been driven primarily by Black and Hispanic students~\cite{nscnews2023}.
                
%                 \item This creates a \textbf{feedback loop}: transfer barriers cause credit loss, imposing burdens that undermine efforts to close equity gaps~\cite{ace2025,nscnews2023}.
                
%                 \item Therefore, automating articulation is not just an operational optimization; it is a \textbf{necessary intervention} to foster educational equity~\cite{collegeopportunity2017}.
%             \end{itemize}
%         \end{column}
    
%         \begin{column}{0.4\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % A circular flow diagram illustrating the "feedback loop"
%             % would be very effective here.
%             %
%             % It could have 3-4 steps:
%             % 1. "Manual Articulation Barriers"
%             % 2. "Credit Loss & Delays"
%             % 3. "Increased Financial & Academic Burden"
%             % 4. "Disproportionate Impact on Underrepresented Students"
%             %
%             % Arrows would connect them in a circle, showing how the
%             % system reinforces inequity.
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for the graphic
            
%         \end{column}
%     \end{columns}

%     \note[item]{These are the very student populations that institutions are striving to support, making transfer efficiency a critical equity lever.}
%     \note[item]{The National Student Clearinghouse reported this trend in Fall 2023, highlighting the increasing diversity of the transfer population.}
%     \note[item]{This troubling loop means the manual system directly counteracts institutional goals of supporting underrepresented students.}
    
% \end{frame}

% % The Goal & My Contribution (Slide 1.5)
% \begin{frame}
%     \frametitle{The Goal \& My Contribution}
%     \vspace{-0.5em}
%     \begin{columns}[T]
%         \begin{column}{0.47\textwidth}
%             \begin{block}{The Goal}
%                 To develop and validate a novel framework that automates course articulation using only publicly available data.
                
%                 \vspace{0.5em}
%                 The resulting system must be:
%                 \begin{itemize}
%                     \item<2-> Accurate \& Scalable
%                     \item<3-> Computationally Efficient
%                     \item<4-> Inherently Privacy-Preserving
%                 \end{itemize}
%             \end{block}

%             % --- SUGGESTED GRAPHIC ---
%             % A simple diagram at the bottom of the slide could unify the concepts.
%             % For example:
%             % [Public Course Catalogs] -> [My Framework] -> [Accurate & Equitable Articulation]
%             % This visually reinforces the input, your process, and the desired outcome.
%             \begin{figure}
%                 \centering
%                 \includegraphics[height=2.3cm]{placeholder.png} % Placeholder for the graphic
%             \end{figure}

%         \end{column}
        
%         \begin{column}{0.47\textwidth}
%             \begin{block}{Primary Contributions}
%                 \hfill
%                 \begin{enumerate}
%                     \item<2-> \textbf{A Highly Accurate Framework:} Developed a complete pipeline achieving state-of-the-art accuracy on real-world data.\vspace{0.5em}
%                     \item<3-> \textbf{An Innovative Feature Vector:} Designed a novel composite vector combining local and global semantics to improve classification.\vspace{0.5em}
%                     \item<4-> \textbf{An Efficient \& Private Approach:} Created a decoupled solution that avoids the high costs and privacy concerns of prior methods.\vspace{1em}
%                 \end{enumerate}
%             \end{block}
%         \end{column}
%     \end{columns}
    
%     \note[item]{For our framework, we achieved F1-scores exceeding 0.97 on the held-out test set.}
%     \note[item]{The composite vector combines the element-wise difference of course embeddings with their cosine similarity, which was shown to be a superior feature set in ablation studies.}
%     \note[item]{Specifically, our approach avoids using sensitive student enrollment data and the high computational cost and opacity of direct LLM classification.}
    
% \end{frame}

% % Agenda (Slide 1.6)
% \begin{frame}
%     \frametitle{Agenda}
%     \tableofcontents
% \end{frame}

% \section{Background \& Related Work}

% % The Landscape of Automation (Slide 2.1)
% \begin{frame}
%     \frametitle{The Landscape of Automation}
    
%     Prior attempts at automation have evolved, with each generation introducing new capabilities while also exposing new limitations.
    
%     \vspace{1em}

%     \footnotesize
%     \renewcommand{\arraystretch}{1.5} % Add vertical space to rows for readability
%     \begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}p{2.7cm} >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X}
%     \toprule
%     \textbf{Approach} & \textbf{Key Characteristic} & \textbf{Core Limitation} \\
%     \midrule
    
%     Keyword \& Statistical (TF-IDF)
%     & Weight terms based on statistical importance~\cite{AIZAWA200345}.
%     & No semantic understanding; cannot grasp synonyms or context. \\
    
%     \addlinespace % Add a little extra space between rows
    
%     Static Embeddings (word2vec, GloVe)
%     & Represent words as averaged, pre-trained vectors.
%     & Context-insensitive, and averaging vectors loses critical semantic information. \\
    
%     \addlinespace
    
%     Enrollment-Based (course2vec)
%     & Learn similarity from student co-enrollment patterns~\cite{PardosCourse2Vec2019}.
%     & Requires sensitive student data, raising major privacy and generalizability issues~\cite{slade10.1177/0002764213479366}. \\
    
%     \addlinespace
    
%     Direct LLM Classification
%     & Use a large language model as an end-to-end classifier.
%     & High computational cost, opaque "black box" reasoning, and sensitive to prompt phrasing~\cite{Errica2024WhatDI}. \\
    
%     \bottomrule
%     \end{tabularx}
    
%     \note{This slide provides the full context for our work. The key takeaway is that each prior method had a significant drawback, whether it was a lack of semantic understanding, a reliance on private data, or operational complexity. This creates a clear research gap that our framework is designed to fill.}

% \end{frame}

% % The Research Gap (Slide 2.2)
% \begin{frame}
%     \frametitle{The Research Gap}
    
%     A review of prior work reveals a fundamental trade-off: as models gain semantic power, they tend to become more computationally intensive, less interpretable, or more demanding of specialized or private data.
    
%     \begin{columns}[T]
%         \begin{column}{0.4\textwidth}
%             \begin{alertblock}{The Opportunity}
%                 The limitations of direct LLM classification (cost, opacity) and enrollment-based methods (privacy, limited access) point toward a gap in the existing research for a new paradigm~\cite{pardos-articulation-2019, slade10.1177/0002764213479366}.
%                 \vspace{1em}
                
%                 \textbf{An effective solution must harness the semantic power of large models without inheriting their operational burdens.}
%             \end{alertblock}
%         \end{column}

%         \begin{column}{0.55\textwidth}
%             \begin{tikzpicture}[scale=0.9, every node/.style={transform shape}, xshift=1em]
%                 % Define styles for the boxes
%                 \tikzstyle{method} = [rectangle, rounded corners, fill=blue!10, text centered, text width=3cm]
%                 \tikzstyle{goal} = [rectangle, rounded corners, fill=green!20, text centered, text width=3cm, font=\bfseries]

%                 % Draw axes
%                 \draw[->, thick] (-3.5,0) -- (3.5,0) node[right] {\begin{tabular}{l}Cost /\\Privacy Risk\end{tabular}};
%                 \draw[->, thick] (0,-2.5) -- (0,2.5) node[above] {Semantic Power};

%                 % Place method nodes
%                 \node[method] at (-1.75, -1.25) {Keyword \& Statistical (TF-IDF)};
                
%                 \node[method] at (1.75, 1.25) {Enrollment-Based \\ Direct LLM};

%                 % Place the goal node
%                 \node[goal] at (-1.75, 1.25) {The Research Gap \\ (Our Goal)};
%             \end{tikzpicture}
%         \end{column}
%     \end{columns}

%     \note{This diagram clearly shows the trade-offs. In the bottom-left, we have simple methods with low semantic power. In the top-right, we have powerful methods that are either expensive or raise privacy concerns. The top-left quadrant is where we want to be: high semantic power, but with low cost and no privacy risk. This is the gap our research addresses.}

% \end{frame}

% \section{A Decoupled Framework for Articulation}

% \begin{frame}
%     \frametitle{Agenda}
%     \tableofcontents[currentsection]
% \end{frame}

% High-Level Architecture (Slide 3.1)
% \begin{frame}
%     \frametitle{A Decoupled Framework: High-Level Architecture}
    
%     Our framework's core principle is to \textbf{decouple rich semantic representation from the final classification task}. This creates a more efficient, scalable, and transparent system.
    
%     \vspace{1em}
    
%     % --- SUGGESTED GRAPHIC ---
%     % A large, clear flowchart is essential for this slide. It should
%     % dominate the frame and visually walk the audience through your entire
%     % process. The flow should be:
%     %
%     % 1. Start with two boxes: "Course A Text" and "Course B Text".
%     %
%     % 2. Arrow to a box: "Deep Embedding Model". Show that this produces
%     %    "Vector A" and "Vector B".
%     %
%     % 3. Arrow to a box: "Feature Engineering". Label this process as
%     %    creating the "Composite Distance Vector (\Delta_c)".
%     %
%     % 4. Arrow to a box: "Traditional ML Classifier (e.g., SVM)".
%     %
%     % 5. Arrow to the final output: "Prediction: Equivalent / Not Equivalent".
%     %
%     % This diagram will be the clearest way to explain your entire methodology.
    
%     \begin{figure}
%         \centering
%         \includegraphics[scale=0.25]{placeholder.png}
%     \end{figure}
    
% \end{frame}

% Core Principle: Decoupling Representation from Classification (Slide 3.2)
% \begin{frame}
%     \frametitle{Core Principle: Decoupling Representation from Classification}

%     By separating the process into two stages, we gain the semantic power of deep learning while avoiding the high operational costs of end-to-end LLM classification~\cite{Errica2024WhatDI} and the privacy risks of enrollment-based methods~\cite{slade10.1177/0002764213479366}.
    
%     \vfill % Center the graphic vertically on the slide
%     \centering
    
%     % --- TikZ Implementation of the Flowchart ---
%     % NOTE: Ensure \usetikzlibrary{arrows.meta, positioning, shapes.geometric} is in your preamble
%     \begin{tikzpicture}[
%         node distance=1cm,
%         % Define styles for the elements
%         stage/.style={rectangle, 
%                       draw, 
%                       rounded corners,
%                       fill=blue!10, 
%                       text width=6cm, 
%                       minimum height=3.5cm, 
%                       align=center},
%         arrow/.style={-Stealth, ultra thick, draw=gray!80}
%     ]
%         % Place the nodes for each stage
%         \node (stage1) [stage] {
%             \textbf{Stage 1: Semantic Representation} \\
%             \textit{(Heavy, Offline Task)}
%             \\[2ex]
%             A deep embedding model converts raw course text into structured, reusable semantic vectors.
%         };
        
%         \node (stage2) [stage, right=of stage1] {
%             \textbf{Stage 2: Pairwise Classification} \\
%             \textit{(Lightweight, Fast Task)}
%             \\[2ex]
%             A traditional ML model performs rapid, on-demand comparisons of the pre-computed vectors.
%         };
        
%         % Draw the arrow connecting the stages
%         \draw [arrow] (stage1.east) -- (stage2.west);
        
%     \end{tikzpicture}
    
%     \vfill
    
%     \note{This is the most important concept in the framework. We do the heavy lifting of understanding language only once, offline. This lets us do the actual classification of pairs incredibly quickly and efficiently. This two-stage process is what gives us the best of both worlds: the semantic power of deep learning without the high operational costs for every comparison, which is what makes the system so scalable and practical.}
% \end{frame}

% % Step 1: Deep Contextual Embeddings (Slide 3.3)
% \begin{frame}
%     \frametitle{Step 1: Deep Contextual Embeddings}

%     The first step is to convert unstructured course catalog text into a structured, semantically rich vector using a pre-trained transformer model~\cite{devlin2019bertpretrainingdeepbidirectional, reimers-2019-sentence-bert}.
    
%     \vfill

%     \begin{figure}
%         \centering
%         \includegraphics[width=\textwidth]{text_to_vector.pdf}
%     \end{figure}

%     \note{The key here is that we're turning unstructured text into a structured, mathematical object—a vector. We use a pre-trained transformer model for this, specifically models from the Sentence-BERT family~\cite{devlin2019bertpretrainingdeepbidirectional, reimers-2019-sentence-bert}. The resulting vector captures the actual meaning of the course, so similar courses will have vectors that are closer together in this high-dimensional space.}

% \end{frame}

% % Step 2: The Composite Distance Vector (Slide 3.4)
% \begin{frame}
%     \fontsize{9}{9}\selectfont
%     \frametitle{Step 2: Our Novel Feature Vector (\(\Delta_c\))}

%     To classify course pairs, we need features that represent the \textit{relationship} between them. We designed a novel \textbf{composite distance vector (\(\Delta_c\))} to provide the classifier with a richer, more discriminative feature set.

%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.55\textwidth}
%             \begin{block}{Combining Local \& Global Information}
%                 The vector combines two distinct types of information:
%                 \begin{itemize}
%                     \item \textbf{Local Disparities:} The granular, element-wise difference between the two course vectors.
%                     \item \textbf{Global Alignment:} A single, holistic score of their overall similarity in the semantic space.
%                 \end{itemize}
%             \end{block}
            
%             \begin{alertblock}{The Formula}
%                 For two \(k\)-dimensional course vectors, \(A\) and \(B\), the composite vector \(\Delta_c\) is constructed by concatenating their element-wise difference with their cosine similarity:
%                 \[ \Delta_{c} = \left( a_{1}-b_{1}, \dots, a_{k}-b_{k}, \frac{A \cdot B}{\parallel A \parallel\parallel B\parallel} \right) \]
%             \end{alertblock}
%         \end{column}

%         \begin{column}{0.4\textwidth}
%             \hfill\\\vspace{0.5em}
%             \centering
%             \includegraphics[width=0.9\textwidth]{composite_distance_vector_generation.pdf} % Placeholder for the graphic
%         \end{column}
%     \end{columns}
% \end{frame}

% % Step 3: Domain-Specific Fine-Tuning (Slide 3.5)
% \begin{frame}
%     \frametitle{Step 3: Domain-Specific Fine-Tuning}
    
%     General-purpose models lack the specialized ``vocabulary'' for academic text. To create a more discriminative embedding space, we fine-tune a pre-trained model on our course data using \textbf{deep metric learning}.
    
%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.55\textwidth}
%             \begin{alertblock}{Learning Objective: The Triplet Loss}
%                 We train the model using a \textbf{Triplet Loss} function, which teaches the model to understand nuanced similarity by operating on triplets of courses~\cite{Schroff_2015_CVPR, hermans2017defensetripletlossperson}:
%                 \begin{itemize}
%                     \item An \textbf{Anchor} course (\(A\))
%                     \item A \textbf{Positive}, equivalent course (\(P\))
%                     \item A \textbf{Negative}, non-equivalent course (\(N\))
%                 \end{itemize}
                
%                 \vspace{0.5em}
%                 The goal is to adjust the embedding space such that the distance between the Anchor and Positive is smaller than the distance between the Anchor and Negative, enforced by a margin (\(\alpha\)):
                
%                 \[ L(A, P, N) = \max\left(d(A, P) - d(A, N) + \alpha, 0\right) \]
%             \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.4\textwidth}
%             \centering
%             \includegraphics[width=0.8\textwidth]{tripletloss_viz_vertical.pdf} % Placeholder for the graphic
%         \end{column}
%     \end{columns}
% \end{frame}

% % Step 4: Downstream Classification (Slide 3.6)
% \begin{frame}
%     \frametitle{Step 4: Downstream Classification}
    
%     \begin{columns}[T]
%         \begin{column}{0.35\textwidth}
%             \hfill\vspace{3mm}
%             \begin{itemize}
%                 \item \textbf{Finalists:}
%                     \begin{itemize}
%                     \item KNN
%                     \item SVM
%                     \item Random Forest
%                     \item XGBoost
%                 \end{itemize}
%                 \vspace{0.5em}
%                 \item \textbf{Key Finding:}\\A clear trade-off emerged
%                 \begin{itemize}
%                     \item \textbf{Most Accurate:}\\\quad SVM
%                     \item \textbf{Most Efficient:}\\\quad Random Forest\\\quad XGBoost
%                 \end{itemize}
%             \end{itemize}
%             % \begin{alertblock}{Classifier Performance}\hfill
%             %     \begin{itemize}
%             %         \item \textbf{Finalists:}
%             %         \begin{itemize}
%             %             \item KNN
%             %             \item SVM
%             %             \item Random Forest
%             %             \item XGBoost
%             %         \end{itemize}
%             %         \vspace{0.5em}
%             %         \item \textbf{Key Finding:}\\A clear trade-off emerged
%             %         \begin{itemize}
%             %             \item \textbf{Most Accurate:}\\\quad SVM
%             %             \item \textbf{Most Efficient:}\\\quad Random Forest\\\quad XGBoost
%             %         \end{itemize}
%             %     \end{itemize}\hfill\vspace{-0.75em}
%             % \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.6\textwidth}
%             \centering
%             \includegraphics[width=\textwidth]{Model_Score_vs_Time_Costs.pdf} % Placeholder for Figure 4.3
%         \end{column}
%     \end{columns}

%     \note[item]{This slide shows the final stage of the pipeline, where we take the engineered feature vectors and feed them into the top four finalist classifiers to get a final prediction.}
%     \note[item]{This visualizes the trade-off between model accuracy on the x-axis, and time cost on a logarithmic y-axis. The left plot is for training time, while the right plot for inference time is the most critical for a real-world system.}
%     \note[item]{Looking at the inference plot on the right, you can see that while all four models are highly accurate—clustered together with F1-scores over 0.97---their speeds are vastly different.}
%     \note[item]{Random Forest and XGBoost are more tightly clustered close to the bottom, showing they are more \textbf{\underbar{consistently}} faster than SVM and KNN. This consistent speed advantage during inference with almost no loss in accuracy makes them the ideal choice for a scalable, low-latency production system.}
%     \note[item]{However, the Support Vector Machine (SVM) is at the very top of the accuracy scale. A formal statistical analysis confirmed it was the most accurate and consistent model, making it the choice where peak performance is the absolute priority.}
% \end{frame}

% \section{Experimental Setup \& Results}

% Data & Evaluation (Slide 4.1)
\begin{frame}
    \frametitle{Data \& Evaluation}
    \fontsize{9}{9}\selectfont
    
    The framework was trained and validated on a real-world dataset to ensure the results are robust and generalizable.
    
    \fontsize{8}{7}\selectfont
    \begin{columns}[T]
        \begin{column}{0.5\textwidth}
            \begin{block}{The PPM Corpus}
                The definitive experiments were conducted on a large corpus provided by the \textbf{Program Pathways Mapper (PPM)}:
                \begin{itemize}
                    \item \textbf{Source:} Real-world data from California's public colleges.
                    \item \textbf{Ground Truth:} Course equivalency is defined by the Course Identification Numbering System (C-ID).
                    \item \textbf{Scale:} The final, cleaned corpus consists of \textbf{2,157 courses} across 157 distinct C-ID classes.
                \end{itemize}
            \end{block}
            
            \vspace{-2mm}
            
            \begin{alertblock}{Rigorous Evaluation Protocol}
                \begin{itemize}
                    \item \textbf{Partitioning:} A stratified \(50/50\) split was used to create non-overlapping training and test sets, ensuring all classes were represented in both.
                    \item \textbf{Primary Metric:} The \textbf{\(F_1\)-Score} was used as the primary metric to balance the trade-off between precision and recall.
                \end{itemize}
            \end{alertblock}
        \end{column}
        
        \begin{column}{0.5\textwidth}
            % --- SUGGESTED GRAPHIC ---
            % A simplified version of Table 4.1 from the thesis would be
            % very effective here to summarize the dataset.
            %
            % You could create a clean, modern-looking table with key stats:
            %
            % | Characteristic | PPM Corpus                     |
            % |----------------|--------------------------------|
            % | Source         | Program Pathways Mapper (PPM)  |
            % | Ground Truth   | C-ID Codes                     |
            % | Final Size     | 2,157 Courses (157 Classes)    |
            % | Partitioning   | Stratified 50/50 Train/Test    |
            %
            % This provides a quick, visual summary of the data foundation.
            
            \centering
            \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for Table 4.1
            
        \end{column}
    \end{columns}
\end{frame}

% % Finding 1: Domain-Specific Fine-Tuning is Critical (Slide 4.2)
% \begin{frame}
%     \frametitle{Finding 1: Domain-Specific Fine-Tuning is Critical}
    
%     Our experiments show that adapting a generic model to the specific language of academia is more effective than relying on sheer model scale.

%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \begin{block}{\small Superior Performance}
%                 \begin{itemize}
%                     \item The fine-tuned model (\textbf{bge-ft}) achieved the highest mean \(F_1\)-score and the lowest variance on the held-out test data.
%                     \item A one-way ANOVA and subsequent Games-Howell post-hoc test confirmed that our fine-tuned model was \textbf{statistically superior} to all off-the-shelf models evaluated.
%                     \item This includes models that were orders of magnitude larger, demonstrating that targeted adaptation is more effective than scale for this specialized task.
%                 \end{itemize}
%             \end{block}
            
%             \begin{alertblock}{\small Key Insight}
%                 For specialized domains, creating a bespoke embedding space through fine-tuning is crucial. It teaches the model the specific semantics and nuances required to make fine-grained distinctions that general-purpose models miss.
%             \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.5\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % A plot showing the validation F1-score over the training
%             % process would be very effective here.
%             %
%             % This would visually echo Figure 4.2 from the thesis,
%             % showing the learning dynamics and how the fine-tuned
%             % model's performance improves and stabilizes.
%             %
%             % Title: "Validation F1-Score During Fine-Tuning"
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for Figure 4.2
            
%         \end{column}
%     \end{columns}
% \end{frame}

% % Finding 2: Final Classifier Performance (Slide 4.3)
% \begin{frame}
%     \frametitle{Finding 2: Final Classifier Performance}
%     \fontsize{9}{9}\selectfont
    
%     The final evaluation, conducted on the held-out test data, measured the efficacy and efficiency of the top-performing classifiers.
    
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \begin{block}{Exceptional Performance Across the Board}
%                 All finalist models (SVM, RF, XGBoost, KNN) achieved exceptionally high and stable performance, with mean \(F_1\)-scores approaching or exceeding \textbf{0.97}.
%             \end{block}

%             \begin{alertblock}{The Accuracy vs. Efficiency Trade-Off}
%                 Our analysis revealed a classic performance trade-off, leading to context-dependent recommendations:
%                 \begin{itemize}
%                     \item \textbf{For Maximum Accuracy:} The \textbf{Support Vector Machine (SVM)} was the statistical winner, proving to be the most accurate and consistent classifier.
                    
%                     \item \textbf{For Optimal Efficiency:} \textbf{Random Forest (RF) and XGBoost} were nearly as accurate but an order of magnitude faster and more predictable at inference time, making them practical choices for a scalable, low-latency system.
%                 \end{itemize}
%             \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.45\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % A scatter plot visualizing the trade-off between
%             % accuracy and inference time would be highly effective.
%             %
%             % This would visually echo Figure 4.3 from the thesis.
%             %
%             % - Y-axis: "F1-Score (Accuracy)"
%             % - X-axis: "Inference Time (seconds) [Log Scale]"
%             %
%             % This plot would clearly show that RF and XGBoost are
%             % clustered on the far left (very fast), while SVM is
%             % slightly to the right (slower) but at the very top
%             % in terms of F1-score.
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for Figure 4.3
            
%         \end{column}
%     \end{columns}
% \end{frame}

% \section{Qualitative Analysis: Beyond the Metrics}

% % Why Do Errors Still Occur? (Slide 5.1)
% \begin{frame}
%     \frametitle{Qualitative Analysis: Why Do Errors Still Occur?}
    
%     With such high accuracy, it's crucial to ask why the system isn't perfect. A purely numerical analysis can be misleading, as aggregate scores can hide systematic failure modes~\cite{gauthier2022}.

%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \begin{block}{Shared Misclassifications Across Models}
%                 Our analysis revealed a high degree of error overlap across all evaluated embedding models, from the smallest to the largest.
%                 \begin{itemize}
%                     \item A core set of \textbf{211 course pairs} were misclassified by \textbf{every single model}.
%                     \item This high count of shared errors points to challenges inherent in the source data itself, not idiosyncratic model weaknesses.
%                 \end{itemize}
%             \end{block}
            
%             \begin{alertblock}{The Primary Bottleneck is Now Data-Centric}
%                 The limiting factor for achieving near-perfect automation is no longer the sophistication of the model. The bottleneck has shifted from being \textbf{model-centric to data-centric}. The model fails when the source data is ambiguous, inconsistent, or lacks a clear textual signal.
%             \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.45\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % The Venn diagram from Figure 4.5 of the thesis would be
%             % the perfect graphic for this slide.
%             %
%             % It visually demonstrates the high degree of overlap in
%             % misclassified pairs between all five embedding models,
%             % with the large number "211" in the central intersection.
%             %
%             % This provides powerful, immediate evidence for the key
%             % message: the errors are systematic and rooted in the data.
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for Figure 4.5
            
%         \end{column}
%     \end{columns}
% \end{frame}

% % Shared Misclassifications (Slide 5.2)
% \begin{frame}
%     \frametitle{Shared Misclassifications: A Data-Centric Problem}
    
%     To diagnose the source of errors, we analyzed their overlap across all embedding models—from the large, general-purpose models to our small, fine-tuned specialist. The results provide strong evidence that the errors are systematic.

%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \begin{block}{The Finding: Models Agree on What's Hard}
%                 A significant portion of failures are not random but are systematic products of the course catalog data itself.
%                 \begin{itemize}
%                     \item A large number of misclassified course pairs were common to all model combinations.
%                     \item This indicates that these "hard" examples consistently challenge a wide range of semantic models.
%                     \item Such errors often arise from annotation artifacts, inherent ambiguity in the source text, or insufficient information to support a clear classification.
%                 \end{itemize}
%             \end{block}

%             \begin{alertblock}{Interpretation}
%                These shared failures strongly suggest that the errors stem from the data itself, not the models. The models are correctly reporting that the texts are not a good match; the issue lies with the ground-truth expectation.
%             \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.45\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % The intersection bar chart from Figure 4.6 of the thesis
%             % would be the perfect graphic for this slide.
%             %
%             % It shows the high counts of shared errors across all
%             % model combinations, reinforcing the message that the
%             % failures are systematic.
%             %
%             % Title: "Number of Common Misclassified Pairs"
            
%             \centering
%             \includegraphics[width=\textwidth]{placeholder.png} % Placeholder for Figure 4.6
            
%         \end{column}
%     \end{columns}
% \end{frame}

% % Root Cause Analysis (Examples) (Slide 5.3)
% \begin{frame}
%     \frametitle{Root Cause Analysis: Common Error Patterns}
    
%     A manual case-based review of the "hard" examples reveals that most errors fall into predictable categories caused by issues in the source data.
    
%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.475\textwidth}
%             \begin{block}{False Negatives (Missed Equivalencies)}
%                 A False Negative occurs when the system fails to identify a true equivalence. These are primarily caused by:
%                 \begin{itemize}
%                     \item \textbf{Semantic Divergence:} Officially equivalent courses are described with vastly different terminology or pedagogical focus. The model correctly assesses the texts as dissimilar; the failure lies in the inconsistent source data.
                    
%                     \item \textbf{Minimalist Descriptions:} One or both course descriptions in a pair are too sparse or incomplete to provide enough textual signal for the model to establish a match.
%                 \end{itemize}
%             \end{block}
%         \end{column}
        
%         \begin{column}{0.475\textwidth}
%             \begin{block}{False Positives (Incorrect Equivalencies)}
%                 A False Positive occurs when two non-equivalent courses are incorrectly approved. These are primarily caused by:
%                 \begin{itemize}
%                     \item \textbf{Topical Overlap:} Courses cover the same broad subject but differ critically in academic level or their position in a sequence (e.g., Physics I vs. Physics II).
                    
%                     \item \textbf{Vague Descriptions:} Descriptions use generic language, lacking the specific detail needed for differentiation. This is a known challenge in short-text semantic similarity~\cite{app13063911}.
%                 \end{itemize}
%             \end{block}
%         \end{column}
%     \end{columns}
    
%     % --- SUGGESTED GRAPHIC ---
%     % A table showing a side-by-side comparison of course descriptions for a specific
%     % False Negative would be a very powerful illustration here.
%     %
%     % For example, use the "Semantic Divergence" case from the thesis (CDEV-100):
%     %
%     % | Course A (Foothill College) | Course B (Cerritos College) |
%     % |-----------------------------|-----------------------------|
%     % | "...examines developmental milestones from conception through adolescence..." | "...examines diversity and inclusion, anti-bias curriculum, and social justice..." |
%     % | **Result: False Negative (Model correctly sees texts as different)** |
%     %
%     % This provides a concrete example of the data-centric problem.
%     \begin{figure}
%         \centering
%         \includegraphics[height=2cm]{placeholder.png}
%     \end{figure}

% \end{frame}

% % The Primary Bottleneck (Slide 5.4)
% \begin{frame}
%     \frametitle{Conclusion of Analysis: The Primary Bottleneck}
    
%     The qualitative analysis leads to a critical insight regarding the future of automated articulation.
    
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
            
%             \begin{alertblock}{The Bottleneck has Shifted from Model-Centric to Data-Centric}
%                 With an optimized pipeline, the limiting factor is no longer the model's architecture or semantic capability.
%                 \begin{itemize}
%                     \item The model is performing correctly; it accurately reports when two texts are not semantically similar.
%                     \item The remaining errors are artifacts of the source data itself: inconsistent descriptions, vague language, and information gaps.
%                     \item Therefore, the most promising path to further improvement lies not in novel architectures, but in methodologies that directly address the quality and consistency of the input data~\cite{gauthier2022}.
%                 \end{itemize}
%             \end{alertblock}
            
%         \end{column}
        
%         \begin{column}{0.45\textwidth}
            
%             % --- SUGGESTED GRAPHIC ---
%             % A visual metaphor diagram would be very effective here to
%             % illustrate the shift in focus.
%             %
%             % It could be a simple flowchart:
%             %
%             % [Model Optimization] --(Green Checkmark)--> [Data Quality]
%             %
%             % The arrow itself could be shaped like a bottleneck, narrowing
%             % as it approaches "Data Quality" to show it's the new constraint.
%             % This visually reinforces that the modeling part is largely solved
%             % and data is the new challenge.
            
%             \centering
%             \includegraphics[width=0.8\textwidth]{placeholder.png}
            
%         \end{column}
%     \end{columns}
    
% \end{frame}

% \section{Conclusion}

% % Limitations (Slide 6.1)
% \begin{frame}
%     \frametitle{Limitations}
    
%     While the proposed framework represents a significant advance, it is essential to acknowledge the boundaries of the current study.
    
%     \begin{block}{Key Limitations}
%         \begin{itemize}
%             \item \textbf{Performance is Capped by Data Quality:} The system's performance is fundamentally limited by the quality and content of the public course descriptions. It cannot infer information that is absent from vague, minimalist, or inconsistent source texts.
            
%             \item \textbf{Generalizability of the Fine-Tuned Model:} The specialized \textit{bge-ft} model was tuned on data from California's public colleges. Its performance may not be as high "out-of-the-box" in other contexts (e.g., private or non-US institutions) without re-tuning on local data.
            
%             \item \textbf{Handling of Complex Articulation Rules:} The framework simplifies articulation into a binary classification of course pairs and does not natively handle complex one-to-many or many-to-many agreements, a challenge that persists for many automated systems~\cite{pardos-articulation-2019}.
%         \end{itemize}
%     \end{block}
    
% \end{frame}

% % Future Work (Slide 6.2)
% \begin{frame}
%     \frametitle{Future Work}
    
%     The findings and limitations of this study give rise to several promising avenues for future research that build upon this work.

%     \fontsize{9}{9}\selectfont
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \begin{block}{Data-Centric AI Strategies}
%                 Since data quality is the primary bottleneck, future work should focus on:
%                 \begin{itemize}
%                     \item Developing an interactive, \textbf{human-in-the-loop} system for expert review of ambiguous pairs.
%                     \item Exploring dynamic data augmentation, such as requesting a full syllabus when classification confidence is low.
%                 \end{itemize}
%             \end{block}
            
%             \begin{block}{Expanding Framework Capabilities}
%                  \begin{itemize}
%                     \item Evolving the framework into a full-scale \textbf{course recommendation engine} with a conversational interface.
%                     \item Investigating graph-based methods (e.g., GNNs) to identify and model complex one-to-many articulation rules.
%                 \end{itemize}
%             \end{block}
%         \end{column}
        
%         \begin{column}{0.5\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % A roadmap or flowchart graphic would be effective here.
%             % It could start with "Current Framework" and branch out
%             % into the different future work directions:
%             %
%             % - Branch 1: "Data-Centric AI"
%             % - Branch 2: "Recommendation Engine"
%             % - Branch 3: "Advanced Modeling"
%             %
%             % This would visually organize the future research paths.
            
%             \centering
%             \includegraphics[width=0.8\textwidth]{placeholder.png}
            
%         \end{column}
%     \end{columns}
% \end{frame}

% % Summary of Contributions (Slide 6.3)
% \begin{frame}
%     \frametitle{Summary of Contributions}
    
%     This research confronted the challenge of manual course articulation by designing, developing, and validating a novel computational framework.
    
%     \begin{columns}[T]
%         \begin{column}{0.5\textwidth}
%             \begin{alertblock}{Primary Contributions}
%                 \begin{enumerate}
%                     \item \textbf{A Novel, Accurate, and Scalable Framework:} We developed an end-to-end pipeline that successfully automates course articulation using only public data, achieving state-of-the-art accuracy.
                    
%                     \item \textbf{Proof that Adaptation Outperforms Scale:} We proved that for this specialized domain, fine-tuning a smaller model for semantic nuance is statistically superior to relying on sheer model scale.
                    
%                     \item \textbf{A Practical Tool for Educational Equity:} We delivered a practical, computationally efficient, and privacy-preserving tool that can reduce administrative burden and help mitigate the systemic inequities faced by transfer students.
%                 \end{enumerate}
%             \end{alertblock}
%         \end{column}
        
%         \begin{column}{0.5\textwidth}
%             % --- SUGGESTED GRAPHIC ---
%             % A high-level impact diagram would work well here.
%             % It could visually summarize the transformation:
%             %
%             % [The Problem: Manual & Inequitable Process]
%             %              |
%             %              V
%             % [Our Framework: Automated & Data-Driven]
%             %              |
%             %              V
%             % [The Outcome: Efficient & Equitable System]
%             %
%             % This provides a simple, memorable summary of the thesis's value.
            
%             \centering
%             \includegraphics[width=0.7\textwidth]{placeholder.png}
            
%         \end{column}
%     \end{columns}
% \end{frame}

% \section{Wrap Up}

% % Thank You & Questions (Slide 7.1)
% \begin{frame}
%     \frametitle{} % No title for a clean look
%     \vfill % Vertically center the content
%     \centering
%     {\Huge \textbf{Thank You!!!}}\\
%     \vspace{5em}
%     {\Huge \textbf{Questions?}}
%     \vfill
% \end{frame}

% % Contact & Acknowledgments (Slide 7.2)
% \begin{frame}
%     \frametitle{Contact \& Acknowledgments}
%     \fontsize{9}{9}\selectfont
    
%     \begin{columns}[T]
%         \begin{column}{0.4\textwidth}
%             \begin{block}{Contact Information}
%                 \textbf{Mark S. Kim}
%                 \begin{itemize}
%                     \item \href{mailto:mkim22@mail.sfsu.edu}{mkim22@mail.sfsu.edu}
%                     % \item \href{https://www.linkedin.com/in/your-profile}{linkedin.com/in/your-profile}
%                     % \item \href{https://github.com/your-repo/project-name}{github.com/your-repo/project-name}
%                 \end{itemize}
%             \end{block}
%         \end{column}
        
%         \begin{column}{0.55\textwidth}
%             \begin{block}{Acknowledgments}
%                 I would like to express my deepest appreciation to:
%                 \begin{itemize}
%                     \item My advisors, Professors \textbf{Hui Yang}, \textbf{Arno Puder}, and \textbf{Anagha Kulkarni}, for their invaluable guidance and support.
%                     \item Professor \textbf{Tao He}, for the crucial suggestion to incorporate a global similarity metric into the feature vector.
%                     \item The \textbf{Program Pathways Mapper (PPM)} team for providing the foundational data for this work.
%                     \item The \textbf{SFSU Academic Technology Systems Team} for their support and the use of the POLARIS High-Performance Computing cluster.
%                 \end{itemize}
%             \end{block}
%         \end{column}
%     \end{columns}
% \end{frame}

% % This frame generates the bibliography at the end of the presentation.
% \begin{frame}[allowframebreaks]
%     \frametitle{References}
%     \printbibliography
% \end{frame}

\end{document}